#!/usr/bin/env python3
"""
Enhanced Academic Report Generator for Random Forest Models

This module generates comprehensive academic reports for Random Forest training experiments,
suitable for research papers and reproducibility documentation. Enhanced with detailed 
data validation reporting, hyperparameter optimization ranges, and comprehensive feature
importance analysis including both tree-based and permutation methods.
"""

import os
import json
import pandas as pd
import numpy as np
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, List, Optional, Union
import logging

logger = logging.getLogger(__name__)


class AcademicReportGenerator:
    """Enhanced academic report generator for Random Forest models with detailed analysis"""
    
    def __init__(self):
        self.report_template = """# Random Forest Training Report

**Generated on:** {timestamp}  
**Model ID:** `{model_id}`  
**Model Folder:** `{model_directory}`

## Executive Summary

This report documents a comprehensive Random Forest training experiment conducted for academic research and reproducibility purposes. The experiment involved{hyperopt_text} cross-validated model training with detailed performance analysis, data validation, and feature importance evaluation using both tree-based and permutation methods.

### Key Results
{key_results}

---

## 1. Experimental Setup

### 1.1 Dataset Information

| Parameter | Value |
|-----------|-------|
{dataset_info}

### 1.2 Training Configuration

| Parameter | Value |
|-----------|-------|
{training_config}

### 1.3 Hardware and Software Environment

- **Python Version:** 3.8+
- **Machine Learning Framework:** scikit-learn
- **Data Processing:** pandas, numpy
- **Hyperparameter Optimization:** Optuna
- **Device:** CPU (with potential multi-threading)

---

## 2. Data Processing and Validation

### 2.1 Data Loading and Initial Inspection

The training data was loaded from `{data_file}` and underwent comprehensive preprocessing to ensure model compatibility and optimal performance.

**Input Features ({n_features} columns):**
{feature_names}

**Target Variables ({n_targets} column{target_plural}):**
{target_names}

{data_validation_section}

### 2.2 Data Preprocessing Pipeline

{preprocessing_section}

### 2.3 Feature Engineering

{feature_engineering_section}

---

{hyperparameter_section}

## {final_section_number}. Final Model Training

### {final_section_number}.1 Cross-Validation Training

The final model was trained using {cv_folds}-fold cross-validation{hyperopt_used}. Training metrics and validation results were recorded comprehensively.

### {final_section_number}.2 Training Results

| Metric | Value |
|--------|-------|
{training_results}

#### Fold-wise Results

{fold_results}

### {final_section_number}.3 Model Performance Visualization

{visualization_section}

### {final_section_number}.4 Feature Importance Analysis

{feature_importance_section}

---

## {arch_section_number}. Model Architecture and Configuration

### {arch_section_number}.1 Random Forest Configuration

The final model uses a Random Forest ensemble with the following specifications:

| Component | Configuration |
|-----------|---------------|
{model_config}

### {arch_section_number}.2 Training Parameters

| Parameter | Value |
|-----------|-------|
{training_params}

---

## {conclusion_section_number}. Conclusions and Future Work

### {conclusion_section_number}.1 Key Findings

{key_findings}

### {conclusion_section_number}.2 Reproducibility

This experiment is fully reproducible using the following artifacts:
{reproducibility_files}

### {conclusion_section_number}.3 Technical Implementation

{technical_implementation}

---

## Appendix

### A.1 System Information

- **Generation Time**: {timestamp}
- **Model ID**: `{model_id}`
- **Training System**: Random Forest MCP Tool
- **Report Version**: 2.0 (Enhanced)

### A.2 File Structure

```
{file_structure}
```

### A.3 Data Files and JSON Artifacts

The following JSON files contain detailed intermediate data for reproducibility:

{json_artifacts_section}

---

*This report was automatically generated by the Enhanced Random Forest MCP Tool for academic research and reproducibility purposes.*
"""

    def generate_report(self, 
                       model_directory: Union[str, Path],
                       model_metadata: Dict[str, Any],
                       training_results: Dict[str, Any],
                       hyperopt_results: Optional[Dict[str, Any]] = None,
                       data_validation_results: Optional[Dict[str, Any]] = None,
                       feature_importance_data: Optional[Dict[str, Any]] = None) -> str:
        """Generate comprehensive academic report with enhanced details"""
        
        model_directory = Path(model_directory)
        model_id = model_metadata.get('model_id', 'unknown')
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        try:
            # Save intermediate data to JSON files
            self._save_intermediate_data(
                model_directory, 
                data_validation_results, 
                hyperopt_results, 
                feature_importance_data,
                training_results
            )
            
            # Collect all necessary information
            report_data = self._collect_report_data(
                model_directory, model_metadata, training_results, hyperopt_results,
                data_validation_results, feature_importance_data
            )
            
            # Generate report content
            report_content = self._format_report(report_data, timestamp, model_id, model_directory)
            
            # Save report
            report_path = model_directory / "academic_report.md"
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write(report_content)
            
            logger.info(f"Enhanced academic report generated: {report_path}")
            return str(report_path)
            
        except Exception as e:
            logger.error(f"Failed to generate enhanced academic report: {e}")
            return ""

    def _save_intermediate_data(self, 
                              model_directory: Path,
                              data_validation_results: Optional[Dict[str, Any]],
                              hyperopt_results: Optional[Dict[str, Any]],
                              feature_importance_data: Optional[Dict[str, Any]],
                              training_results: Dict[str, Any]) -> None:
        """Save intermediate data to JSON files for reproducibility"""
        
        # Create reports subdirectory
        reports_dir = model_directory / "reports"
        reports_dir.mkdir(exist_ok=True)
        
        try:
            # Save data validation results
            if data_validation_results:
                validation_path = reports_dir / "data_validation_report.json"
                with open(validation_path, 'w', encoding='utf-8') as f:
                    json.dump(data_validation_results, f, indent=2, default=str)
                logger.info(f"Data validation report saved: {validation_path}")
            
            # Save hyperparameter optimization details
            if hyperopt_results:
                hyperopt_path = reports_dir / "hyperparameter_optimization.json"
                # Include parameter ranges for documentation
                enhanced_hyperopt = hyperopt_results.copy()
                enhanced_hyperopt['parameter_ranges'] = self._get_hyperparameter_ranges()
                with open(hyperopt_path, 'w', encoding='utf-8') as f:
                    json.dump(enhanced_hyperopt, f, indent=2, default=str)
                logger.info(f"Hyperparameter optimization report saved: {hyperopt_path}")
            
            # Save feature importance analysis
            if feature_importance_data:
                importance_path = reports_dir / "feature_importance_analysis.json"
                with open(importance_path, 'w', encoding='utf-8') as f:
                    json.dump(feature_importance_data, f, indent=2, default=str)
                logger.info(f"Feature importance analysis saved: {importance_path}")
            
            # Save training summary
            training_summary_path = reports_dir / "training_summary.json"
            with open(training_summary_path, 'w', encoding='utf-8') as f:
                json.dump(training_results, f, indent=2, default=str)
            logger.info(f"Training summary saved: {training_summary_path}")
            
        except Exception as e:
            logger.warning(f"Failed to save some intermediate data: {e}")

    def _get_hyperparameter_ranges(self) -> Dict[str, Any]:
        """Get the hyperparameter search ranges used in optimization"""
        return {
            "n_estimators": {
                "range": [50, 300],
                "step": 25,
                "description": "Number of trees in the forest"
            },
            "max_depth": {
                "range": [3, 20],
                "step": 1,
                "description": "Maximum depth of the tree"
            },
            "min_samples_split": {
                "range": [2, 20],
                "step": 1,
                "description": "Minimum number of samples required to split an internal node"
            },
            "min_samples_leaf": {
                "range": [1, 10],
                "step": 1,
                "description": "Minimum number of samples required to be at a leaf node"
            },
            "max_features": {
                "options": ["sqrt", "log2", 0.5, 0.7, 1.0],
                "description": "Number of features to consider when looking for the best split"
            },
            "bootstrap": {
                "options": [True, False],
                "description": "Whether bootstrap samples are used when building trees"
            }
        }

    def _collect_report_data(self, 
                           model_directory: Path,
                           model_metadata: Dict[str, Any],
                           training_results: Dict[str, Any],
                           hyperopt_results: Optional[Dict[str, Any]],
                           data_validation_results: Optional[Dict[str, Any]],
                           feature_importance_data: Optional[Dict[str, Any]]) -> Dict[str, Any]:
        """Collect report data including validation and feature importance"""
        
        data = {
            'model_directory': model_directory,
            'model_metadata': model_metadata,
            'training_results': training_results,
            'hyperopt_results': hyperopt_results,
            'data_validation_results': data_validation_results,
            'feature_importance_data': feature_importance_data,
            'has_hyperopt': hyperopt_results is not None,
            'has_validation': data_validation_results is not None,
            'has_feature_importance': feature_importance_data is not None,
            'files': self._scan_model_files(model_directory)
        }
        
        return data

    def _format_report(self, data: Dict[str, Any], timestamp: str, model_id: str, model_directory: Path) -> str:
        """æ ¼å¼åŒ–æŠ¥å‘Šå†…å®¹"""
        
        # åŸºæœ¬ä¿¡æ¯
        has_hyperopt = data['has_hyperopt']
        metadata = data['model_metadata']
        results = data['training_results']
        
        # åŠ¨æ€è°ƒæ•´ç« èŠ‚ç¼–å·
        if has_hyperopt:
            hyperparameter_section = "## 3. Hyperparameter Optimization\n\n" + self._format_hyperopt_section(data['hyperopt_results'])
            final_section_number = "4"
            arch_section_number = "5"
            conclusion_section_number = "6"
        else:
            hyperparameter_section = ""
            final_section_number = "3"
            arch_section_number = "4"
            conclusion_section_number = "5"
        
        # æ ¼å¼åŒ–å„ä¸ªéƒ¨åˆ†
        format_params = {
            'timestamp': timestamp,
            'model_id': model_id,
            'model_directory': str(model_directory),
            'hyperopt_text': " hyperparameter optimization and" if has_hyperopt else "",
            'key_results': self._format_key_results(model_directory),
            'dataset_info': self._format_dataset_info(metadata),
            'training_config': self._format_training_config(metadata),
            'data_file': metadata.get('data_source', 'N/A'),
            'n_features': metadata.get('n_features', 'N/A'),
            'n_targets': metadata.get('target_dimension', metadata.get('n_targets', 1)),
            'target_plural': 's' if metadata.get('target_dimension', metadata.get('n_targets', 1)) > 1 else '',
            'feature_names': self._format_feature_names(metadata.get('feature_names', [])),
            'target_names': self._format_target_names(metadata.get('target_name', metadata.get('target_names', []))),
            'data_validation_section': self._format_data_validation_section(data.get('data_validation_results')),
            'preprocessing_section': self._format_preprocessing_section(metadata),
            'feature_engineering_section': self._format_feature_engineering_section(metadata),
            'hyperparameter_section': hyperparameter_section,
            'final_section_number': final_section_number,
            'cv_folds': metadata.get('cv_folds', 5),
            'hyperopt_used': " with optimized hyperparameters" if has_hyperopt else "",
            'training_results': self._format_training_results(results),
            'fold_results': self._format_fold_results(results),
            'visualization_section': self._format_visualization_section(data['files'], metadata),
            'feature_importance_section': self._format_enhanced_feature_importance_section(
                data.get('feature_importance_data'), data['files'], metadata
            ),
            'arch_section_number': arch_section_number,
            'model_config': self._format_model_config(metadata),
            'training_params': self._format_training_params(metadata),
            'conclusion_section_number': conclusion_section_number,
            'key_findings': self._format_key_findings(results, metadata, has_hyperopt),
            'reproducibility_files': self._format_reproducibility_files(data['files'], model_directory),
            'technical_implementation': self._format_technical_implementation(),
            'file_structure': self._format_file_structure(data['files'], model_directory),
            'json_artifacts_section': self._format_json_artifacts_section(data['files'], model_directory)
        }
        
        return self.report_template.format(**format_params)

    def _format_key_results(self, model_directory: Path) -> str:
        """æ ¼å¼åŒ–å…³é”®ç»“æœ - ç›´æ¥ä»ä¿å­˜çš„æ–‡ä»¶ä¸­è¯»å–æ•°æ®"""
        lines = []
        
        try:
            # è¯»å–äº¤å‰éªŒè¯ç»“æœ
            cv_results_file = model_directory / "cross_validation_results.json"
            metadata_file = model_directory / "metadata.json"
            
            if cv_results_file.exists():
                with open(cv_results_file, 'r', encoding='utf-8') as f:
                    cv_data = json.load(f)
                
                task_type = cv_data.get('task_type', 'unknown')
                test_scores = cv_data.get('test_scores', {})
                
                if test_scores:
                    lines.append("### ğŸ¯ å…³é”®æ€§èƒ½æŒ‡æ ‡")
                    lines.append("")
                    
                    if task_type == 'classification':
                        # åˆ†ç±»ä»»åŠ¡ï¼šæ˜¾ç¤ºå‡†ç¡®ç‡ã€F1åˆ†æ•°ã€ç²¾ç¡®ç‡ã€å¬å›ç‡
                        classification_metrics = [
                            ('ACCURACY', 'å‡†ç¡®ç‡ (Accuracy)'),
                            ('F1', 'F1åˆ†æ•° (F1 Score)'),
                            ('PRECISION', 'ç²¾ç¡®ç‡ (Precision)'),
                            ('RECALL', 'å¬å›ç‡ (Recall)')
                        ]
                        
                        for metric_key, metric_name in classification_metrics:
                            if metric_key in test_scores:
                                metric_data = test_scores[metric_key]
                                mean_score = metric_data.get('mean', 0)
                                std_score = metric_data.get('std', 0)
                                lines.append(f"- **{metric_name}:** {mean_score:.6f} (Â±{std_score:.6f})")
                    
                    elif task_type == 'regression':
                        # å›å½’ä»»åŠ¡ï¼šæ˜¾ç¤ºRÂ²ã€MAEã€MSEã€RMSE
                        regression_metrics = [
                            ('R2', 'RÂ²åˆ†æ•° (RÂ² Score)'),
                            ('MAE', 'å¹³å‡ç»å¯¹è¯¯å·® (Mean Absolute Error)'),
                            ('MSE', 'å‡æ–¹è¯¯å·® (Mean Squared Error)'),
                            ('RMSE', 'å‡æ–¹æ ¹è¯¯å·® (Root Mean Squared Error)')
                        ]
                        
                        for metric_key, metric_name in regression_metrics:
                            if metric_key in test_scores:
                                metric_data = test_scores[metric_key]
                                mean_score = metric_data.get('mean', 0)
                                std_score = metric_data.get('std', 0)
                                lines.append(f"- **{metric_name}:** {mean_score:.6f} (Â±{std_score:.6f})")
                    
                    else:
                        # æœªçŸ¥ä»»åŠ¡ç±»å‹ï¼šæ˜¾ç¤ºæ‰€æœ‰å¯ç”¨æŒ‡æ ‡
                        for metric_key, metric_data in test_scores.items():
                            if isinstance(metric_data, dict) and 'mean' in metric_data:
                                mean_score = metric_data.get('mean', 0)
                                std_score = metric_data.get('std', 0)
                                lines.append(f"- **{metric_key}:** {mean_score:.6f} (Â±{std_score:.6f})")
                    
                    lines.append("")
                    
                    # æ·»åŠ äº¤å‰éªŒè¯ä¿¡æ¯
                    cv_folds = cv_data.get('cv_folds', 'N/A')
                    lines.append(f"- **äº¤å‰éªŒè¯æŠ˜æ•°:** {cv_folds}")
                    
                    # æ·»åŠ æ•°æ®é›†ä¿¡æ¯
                    data_shape = cv_data.get('data_shape', [])
                    if len(data_shape) >= 2:
                        lines.append(f"- **æ•°æ®é›†è§„æ¨¡:** {data_shape[0]} æ ·æœ¬, {data_shape[1]} ç‰¹å¾")
            
            # è¯»å–è¶…å‚æ•°ä¼˜åŒ–ç»“æœ
            if metadata_file.exists():
                with open(metadata_file, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                
                hyperparams = metadata.get('hyperparameters', {})
                if hyperparams:
                    lines.append("")
                    lines.append("### âš™ï¸ æœ€ä¼˜è¶…å‚æ•°")
                    lines.append("")
                    for param, value in hyperparams.items():
                        lines.append(f"- **{param}:** {value}")
                
                # æ·»åŠ è®­ç»ƒæ—¶é—´
                training_time = metadata.get('training_time_seconds', 0)
                if training_time > 0:
                    lines.append("")
                    lines.append(f"- **è®­ç»ƒæ—¶é—´:** {training_time:.2f} ç§’")
        
        except Exception as e:
            lines.append(f"âš ï¸ æ— æ³•è¯»å–å…³é”®ç»“æœæ•°æ®: {str(e)}")
        
        return "\n".join(lines) if lines else "âš ï¸ æœªæ‰¾åˆ°å…³é”®ç»“æœæ•°æ®"

    def _format_dataset_info(self, metadata: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–æ•°æ®é›†ä¿¡æ¯"""
        lines = []
        
        if 'data_source' in metadata:
            lines.append(f"| Data File | `{metadata['data_source']}` |")
        if 'data_shape' in metadata:
            shape = metadata['data_shape']
            lines.append(f"| Data Shape | {shape} |")
        if 'n_features' in metadata:
            lines.append(f"| Number of Features | {metadata['n_features']} |")
        
        # Handle target dimension correctly 
        if 'target_dimension' in metadata:
            lines.append(f"| Number of Targets | {metadata['target_dimension']} |")
        elif 'n_targets' in metadata:
            lines.append(f"| Number of Targets | {metadata['n_targets']} |")
        else:
            # Fallback: try to determine from target_column
            target_col = metadata.get('target_column', [])
            if isinstance(target_col, list):
                lines.append(f"| Number of Targets | {len(target_col)} |")
            else:
                lines.append(f"| Number of Targets | 1 |")
                
        if 'n_samples' in metadata:
            lines.append(f"| Total Samples | {metadata['n_samples']} |")
        
        return '\n'.join(lines)

    def _format_training_config(self, metadata: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–è®­ç»ƒé…ç½®"""
        lines = []
        
        if 'algorithm' in metadata:
            lines.append(f"| Algorithm | {metadata['algorithm']} |")
        if 'cv_folds' in metadata:
            lines.append(f"| Cross-Validation Folds | {metadata['cv_folds']} |")
        if 'random_state' in metadata:
            lines.append(f"| Random State | {metadata['random_state']} |")
        if 'task_type' in metadata:
            lines.append(f"| Task Type | {metadata['task_type'].title()} |")
        
        return '\n'.join(lines)

    def _format_feature_names(self, feature_names: List[str]) -> str:
        """æ ¼å¼åŒ–ç‰¹å¾åç§°"""
        if not feature_names:
            return "Feature names not available"
        return ', '.join([f"`{name}`" for name in feature_names])

    def _format_target_names(self, target_names: List[str]) -> str:
        """æ ¼å¼åŒ–ç›®æ ‡åç§°"""
        if not target_names:
            return "Target names not available"
        return ', '.join([f"`{name}`" for name in target_names])

    def _format_preprocessing_section(self, metadata: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–é¢„å¤„ç†éƒ¨åˆ†"""
        preprocessing_applied = metadata.get('preprocessing_applied', False)
        
        if not preprocessing_applied:
            return """The data preprocessing pipeline maintains data in its original form to preserve interpretability and domain-specific meaning.

#### 2.2.1 Minimal Preprocessing Approach

**Strategy**: Preserve original data characteristics

Random Forest models are robust to feature scaling and can handle:
- **Mixed Data Types**: Numerical and categorical features without transformation
- **Different Scales**: Features with varying ranges and units
- **Non-linear Relationships**: Complex patterns without feature engineering

**Benefits of Minimal Preprocessing:**
- **Interpretability**: Results directly relate to original feature meanings
- **Robustness**: Less sensitive to preprocessing parameter choices
- **Simplicity**: Reduced pipeline complexity and potential failure points
- **Domain Knowledge**: Preserves expert understanding of feature relationships"""
        else:
            return """The data underwent comprehensive preprocessing to optimize model performance and ensure consistent data quality.

#### 2.2.1 Feature Preprocessing

**Preprocessing Method**: StandardScaler (Z-score normalization)

```python
# Feature transformation: X_scaled = (X - Î¼) / Ïƒ
# Where Î¼ = mean, Ïƒ = standard deviation
X_scaled = (X - X.mean(axis=0)) / X.std(axis=0)
```

**Preprocessing Benefits:**
- **Feature Consistency**: Normalizes different scales and units
- **Algorithm Optimization**: Improves convergence for distance-based methods
- **Numerical Stability**: Prevents overflow/underflow in computations
- **Cross-Validation Integrity**: Separate scaling per fold prevents data leakage"""

    def _format_feature_engineering_section(self, metadata: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–ç‰¹å¾å·¥ç¨‹éƒ¨åˆ†"""
        return """### 2.3 Feature Selection and Engineering

#### 2.3.1 Feature Selection Strategy

**Approach**: Comprehensive feature utilization

Random Forest inherently performs feature selection through:
- **Random Subspace Method**: Each tree uses random feature subsets
- **Feature Importance Calculation**: Automatic relevance scoring
- **Ensemble Averaging**: Reduces impact of irrelevant features

#### 2.3.2 Feature Engineering Pipeline

**Current Features**: All original features retained for maximum information preservation
**Categorical Encoding**: Automatic handling by Random Forest algorithm
**Missing Value Strategy**: Handled by tree-based splitting criteria
**Feature Interaction**: Captured implicitly through tree structure"""

    def _format_hyperopt_section(self, hyperopt_results: Dict[str, Any]) -> str:
        """Enhanced hyperparameter optimization section with detailed ranges and analysis"""
        if not hyperopt_results:
            return ""
        
        # Get parameter ranges
        param_ranges = self._get_hyperparameter_ranges()
        
        lines = [
            "### 3.1 Hyperparameter Search Space",
            "",
            "The optimization process systematically explored a comprehensive parameter space designed to balance model complexity and performance:",
            "",
            "| Parameter | Range/Options | Description |",
            "|-----------|---------------|-------------|"
        ]
        
        # Add detailed parameter ranges
        for param, config in param_ranges.items():
            if 'range' in config:
                range_str = f"{config['range'][0]}-{config['range'][1]}"
                if 'step' in config:
                    range_str += f" (step: {config['step']})"
            else:
                range_str = str(config['options'])
            lines.append(f"| {param} | {range_str} | {config['description']} |")
        
        lines.extend([
            "",
            "### 3.2 Optimization Algorithm and Strategy",
            "",
            f"**Algorithm**: {str(hyperopt_results.get('sampler_type') or 'TPE').upper()} (Tree-structured Parzen Estimator)",
            f"**Total Trials**: {hyperopt_results.get('n_trials', 'N/A')}",
            f"**Completed Trials**: {hyperopt_results.get('n_completed_trials', 'N/A')}",
            f"**Best Score**: {hyperopt_results.get('best_score', 'N/A'):.6f}",
            "",
            "**Optimization Strategy:**",
            "- **Initial Exploration**: 10 random trials for space exploration",
            "- **Exploitation-Exploration Balance**: TPE algorithm balances promising regions with unexplored space",
            "- **Cross-Validation**: Each trial evaluated using stratified k-fold cross-validation",
            "- **Early Stopping**: Poor-performing trials terminated early to improve efficiency",
            "",
            "### 3.3 Best Parameters Found",
            "",
            "```json",
            json.dumps(hyperopt_results.get('best_params', {}), indent=2),
            "```",
            "",
            "### 3.4 Optimization Convergence",
            "",
            f"The optimization process completed **{hyperopt_results.get('n_trials', 'N/A')} trials** with the best configuration achieving a cross-validation score of **{hyperopt_results.get('best_score', 'N/A'):.6f}**.",
            "",
            "**Key Optimization Insights:**"
        ])
        
        # Add parameter-specific insights if available
        best_params = hyperopt_results.get('best_params', {})
        if best_params:
            lines.extend([
                f"- **Forest Size**: {best_params.get('n_estimators', 'N/A')} trees balances performance and computational efficiency",
                f"- **Tree Depth**: Maximum depth of {best_params.get('max_depth', 'N/A')} prevents overfitting while capturing complexity",
                f"- **Sampling Strategy**: Bootstrap = {best_params.get('bootstrap', 'N/A')} for {'ensemble diversity' if best_params.get('bootstrap') else 'deterministic splitting'}",
                f"- **Feature Selection**: {best_params.get('max_features', 'N/A')} features per split optimizes randomness vs. performance"
            ])
        
        return '\n'.join(lines)

    def _format_training_results(self, results: Dict[str, Any]) -> str:
        """Format comprehensive training results including multiple metrics"""
        lines = []
        
        # Check for detailed test scores (MAE, MSE, R2, etc.)
        if 'test_scores' in results:
            test_scores = results['test_scores']
            lines.append("### Cross-Validation Performance Metrics")
            lines.append("")
            lines.append("| Metric | Mean Â± Std | Min | Max |")
            lines.append("|--------|------------|-----|-----|")
            
            for metric_name, metric_data in test_scores.items():
                if isinstance(metric_data, dict):
                    mean_val = metric_data.get('mean', 0)
                    std_val = metric_data.get('std', 0)
                    min_val = metric_data.get('min', 0)
                    max_val = metric_data.get('max', 0)
                    
                    # Format metric name - ensure it's a string
                    formatted_metric = str(metric_name or 'Unknown').replace('_', ' ').upper()
                    
                    lines.append(f"| {formatted_metric} | {mean_val:.6f} Â± {std_val:.6f} | {min_val:.6f} | {max_val:.6f} |")
            
            lines.extend(["", ""])
        
        # Fallback to basic CV scores if detailed scores not available
        elif 'cv_scores' in results:
            cv_scores = results['cv_scores']
            if isinstance(cv_scores, dict) and 'test_score' in cv_scores:
                scores = cv_scores['test_score']
                lines.append("| Average CV Score | {:.6f} |".format(np.mean(scores)))
                lines.append("| CV Standard Deviation | {:.6f} |".format(np.std(scores)))
                lines.append("| Best Fold Score | {:.6f} |".format(np.max(scores)))
                lines.append("| Worst Fold Score | {:.6f} |".format(np.min(scores)))
        
        # Add final model score if available
        if 'model_score' in results:
            lines.append(f"| Final Model Score | {results['model_score']:.6f} |")
        
        # Add training time information
        if 'training_time_seconds' in results:
            training_time = results['training_time_seconds']
            lines.append(f"| Training Time | {training_time:.2f} seconds |")
        
        return '\n'.join(lines) if lines else "| Training Status | Completed Successfully |"

    def _format_fold_results(self, results: Dict[str, Any]) -> str:
        """Format detailed fold-wise results for all metrics"""
        
        # Check for detailed test scores first
        if 'test_scores' in results:
            test_scores = results['test_scores']
            
            # Create comprehensive fold results table
            lines = ["#### Detailed Fold-wise Performance", ""]
            
            # Get the number of folds from any metric
            n_folds = 0
            first_metric = None
            for metric_name, metric_data in test_scores.items():
                if isinstance(metric_data, dict) and 'scores' in metric_data:
                    n_folds = len(metric_data['scores'])
                    first_metric = metric_name
                    break
            
            if n_folds > 0:
                # Create header
                header = "| Fold |"
                separator = "|------|"
                
                for metric_name in test_scores.keys():
                    formatted_metric = str(metric_name or 'Unknown').replace('_', ' ').upper()
                    header += f" {formatted_metric} |"
                    separator += "---------|"
                
                lines.extend([header, separator])
                
                # Create rows for each fold
                for fold_idx in range(n_folds):
                    row = f"| {fold_idx + 1} |"
                    
                    for metric_name, metric_data in test_scores.items():
                        if isinstance(metric_data, dict) and 'scores' in metric_data:
                            score = metric_data['scores'][fold_idx]
                            row += f" {score:.6f} |"
                        else:
                            row += " N/A |"
                    
                    lines.append(row)
                
                # Add statistical summary
                lines.extend(["", "#### Statistical Summary", ""])
                lines.extend([
                    "| Metric | Mean | Std Dev | Min | Max | 95% CI |",
                    "|--------|------|---------|-----|-----|--------|"
                ])
                
                for metric_name, metric_data in test_scores.items():
                    if isinstance(metric_data, dict) and 'scores' in metric_data:
                        scores = metric_data['scores']
                        mean_score = np.mean(scores)
                        std_score = np.std(scores)
                        min_score = np.min(scores)
                        max_score = np.max(scores)
                        
                        # Calculate 95% confidence interval
                        ci_margin = 1.96 * std_score / np.sqrt(len(scores))
                        ci_lower = mean_score - ci_margin
                        ci_upper = mean_score + ci_margin
                        
                        formatted_metric = str(metric_name or 'Unknown').replace('_', ' ').upper()
                        lines.append(
                            f"| {formatted_metric} | {mean_score:.6f} | {std_score:.6f} | "
                            f"{min_score:.6f} | {max_score:.6f} | [{ci_lower:.6f}, {ci_upper:.6f}] |"
                        )
                
                return '\n'.join(lines)
        
        # Fallback to basic CV scores
        if 'cv_scores' not in results:
            return "Fold-wise results not available"
        
        cv_scores = results['cv_scores']
        if not isinstance(cv_scores, dict) or 'test_score' not in cv_scores:
            return "Fold-wise results not available"
        
        scores = cv_scores['test_score']
        lines = ["| Fold | Score |", "|------|-------|"]
        
        for i, score in enumerate(scores, 1):
            lines.append(f"| {i} | {score:.6f} |")
        
        return '\n'.join(lines)

    def _format_visualization_section(self, files: Dict[str, List[str]], metadata: Dict[str, Any] = None) -> str:
        """æ ¼å¼åŒ–å¯è§†åŒ–éƒ¨åˆ†"""
        section = ""
        
        # Get task type from metadata
        task_type = metadata.get('task_type', 'regression') if metadata else 'regression'
        
        # æ£€æŸ¥äº¤å‰éªŒè¯æ•£ç‚¹å›¾æ˜¯å¦åœ¨cross_validation_dataå­ç›®å½•ä¸­
        has_cv_scatter = False
        cv_scatter_path = "cross_validation_scatter.png"
        
        # é¦–å…ˆæ£€æŸ¥cross_validation_dataå­ç›®å½•
        if 'cross_validation_scatter.png' in files.get('cv_data', []):
            has_cv_scatter = True
            cv_scatter_path = "cross_validation_data/cross_validation_scatter.png"
        # ç„¶åæ£€æŸ¥æ ¹ç›®å½•ï¼ˆå‘åå…¼å®¹ï¼‰
        elif 'cross_validation_scatter.png' in files.get('plots', []):
            has_cv_scatter = True
            cv_scatter_path = "cross_validation_scatter.png"
        
        # æ£€æŸ¥å½’ä¸€åŒ–æ•£ç‚¹å›¾ - åªå¯¹å›å½’ä»»åŠ¡æ˜¾ç¤º
        has_normalized_scatter = False
        normalized_scatter_path = ""
        if (task_type == "regression" and 
            'cross_validation_scatter_normalized.png' in files.get('cv_data', [])):
            has_normalized_scatter = True
            normalized_scatter_path = "cross_validation_data/cross_validation_scatter_normalized.png"
        
        # æ£€æŸ¥åŸå§‹å°ºåº¦æ•£ç‚¹å›¾ - åªå¯¹å›å½’ä»»åŠ¡æ˜¾ç¤º
        has_original_scatter = False
        original_scatter_path = ""
        if (task_type == "regression" and 
            'cross_validation_scatter_original.png' in files.get('cv_data', [])):
            has_original_scatter = True
            original_scatter_path = "cross_validation_data/cross_validation_scatter_original.png"
        
        # æ£€æŸ¥ROCæ›²çº¿ - åªå¯¹åˆ†ç±»ä»»åŠ¡æ˜¾ç¤º
        has_roc_curves = False
        roc_curves_path = ""
        if (task_type == "classification" and 
            'cross_validation_roc_curves.png' in files.get('cv_data', [])):
            has_roc_curves = True
            roc_curves_path = "cross_validation_data/cross_validation_roc_curves.png"
        
        # å¦‚æœæ‰¾åˆ°äº¤å‰éªŒè¯æ•£ç‚¹å›¾ï¼ˆå›å½’ä»»åŠ¡ï¼‰
        if has_cv_scatter and task_type == "regression":
            section += f"""#### Training Performance Analysis

The cross-validation analysis demonstrates the model's predictive performance through scatter plots comparing predicted versus actual values.

<div style="text-align: center; margin: 20px 0;">
    <img src="{cv_scatter_path}" alt="Cross-Validation Scatter Plot" style="max-width: 100%; height: auto; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
    <p style="font-style: italic; color: #666; margin-top: 10px;">Cross-Validation: Predicted vs Actual Values</p>
</div>

"""
        
        # æ·»åŠ å½’ä¸€åŒ–æ•£ç‚¹å›¾ï¼ˆä»…å›å½’ä»»åŠ¡ï¼‰
        if has_normalized_scatter:
            section += f"""
<div style="text-align: center; margin: 20px 0;">
    <img src="{normalized_scatter_path}" alt="Normalized Cross-Validation Scatter Plot" style="max-width: 100%; height: auto; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
    <p style="font-style: italic; color: #666; margin-top: 10px;">Cross-Validation Results on Normalized Data</p>
</div>

"""
        
        # æ·»åŠ åŸå§‹å°ºåº¦æ•£ç‚¹å›¾ï¼ˆä»…å›å½’ä»»åŠ¡ï¼‰
        if has_original_scatter:
            section += f"""
<div style="text-align: center; margin: 20px 0;">
    <img src="{original_scatter_path}" alt="Original Scale Cross-Validation Scatter Plot" style="max-width: 100%; height: auto; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
    <p style="font-style: italic; color: #666; margin-top: 10px;">Cross-Validation Results on Original Scale</p>
</div>

"""
        
        # æ·»åŠ ROCæ›²çº¿ï¼ˆä»…åˆ†ç±»ä»»åŠ¡ï¼‰
        if has_roc_curves:
            section += f"""#### Classification Performance Analysis

The cross-validation analysis demonstrates the model's classification performance through ROC curves showing the trade-off between true positive rate and false positive rate.

<div style="text-align: center; margin: 20px 0;">
    <img src="{roc_curves_path}" alt="Cross-Validation ROC Curves" style="max-width: 100%; height: auto; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
    <p style="font-style: italic; color: #666; margin-top: 10px;">Cross-Validation ROC Curves</p>
</div>

"""
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç›¸åº”ä»»åŠ¡ç±»å‹çš„å¯è§†åŒ–å›¾
        if task_type == "regression" and not has_cv_scatter and not has_normalized_scatter and not has_original_scatter:
            section += """#### Training Performance Analysis

<div class="info-box" style="background-color: #e7f3ff; border-left: 4px solid #2196F3; padding: 15px; margin: 15px 0; border-radius: 4px;">
    <strong>ğŸ“Š Note:</strong> Cross-validation scatter plot visualizations would be displayed here when available.
</div>

"""
        elif task_type == "classification" and not has_roc_curves:
            section += """#### Classification Performance Analysis

<div class="info-box" style="background-color: #e7f3ff; border-left: 4px solid #2196F3; padding: 15px; margin: 15px 0; border-radius: 4px;">
    <strong>ğŸ“Š Note:</strong> Cross-validation ROC curve visualizations would be displayed here when available.
</div>

"""
        
        # æ£€æŸ¥å­¦ä¹ æ›²çº¿
        has_learning_curve = False
        learning_curve_path = "learning_curve.png"
        
        # æ£€æŸ¥cross_validation_dataå­ç›®å½•ä¸­çš„å­¦ä¹ æ›²çº¿
        if 'learning_curve.png' in files.get('cv_data', []):
            has_learning_curve = True
            learning_curve_path = "cross_validation_data/learning_curve.png"
        # å‘åå…¼å®¹ï¼šæ£€æŸ¥æ ¹ç›®å½•
        elif 'learning_curve.png' in files.get('plots', []):
            has_learning_curve = True
            learning_curve_path = "learning_curve.png"
        
        if has_learning_curve:
            section += f"""#### Learning Curve Analysis

![Learning Curve]({learning_curve_path})

*Figure: Model learning curve showing training and validation performance across different training set sizes. This helps assess model bias and variance, and determines if more data would improve performance.*

"""
        
        return section

    def _format_feature_importance_section(self, files: Dict[str, List[str]], metadata: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–ç‰¹å¾é‡è¦æ€§éƒ¨åˆ†"""
        section = """#### Feature Importance Analysis

Random Forest provides natural feature importance metrics through:

**Importance Metrics:**
- **Mean Decrease in Impurity**: Average reduction in node impurity
- **Permutation Importance**: Impact of shuffling feature values
- **Split Frequency**: How often features are selected for splits

"""
        
        # ç‰¹å¾é‡è¦æ€§å›¾
        if 'feature_importance.png' in files.get('plots', []):
            section += """![Feature Importance](feature_importance.png)

**File:** `feature_importance.png`

This plot shows relative importance of all features in the model.

"""
        
        # ç‰¹å¾é‡è¦æ€§CSV
        if 'feature_importance.csv' in files.get('data', []):
            section += """**Feature Importance Data:**
- `feature_importance.csv` - Detailed feature importance scores

"""
        
        return section

    def _format_model_config(self, metadata: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–æ¨¡å‹é…ç½®"""
        lines = []
        
        # ä»metadataä¸­æå–Random Forestå‚æ•°
        rf_params = metadata.get('model_params', {})
        
        lines.append(f"| Number of Trees | {rf_params.get('n_estimators', 'Default')} |")
        lines.append(f"| Max Depth | {rf_params.get('max_depth', 'None (unlimited)')} |")
        lines.append(f"| Min Samples Split | {rf_params.get('min_samples_split', 'Default')} |")
        lines.append(f"| Min Samples Leaf | {rf_params.get('min_samples_leaf', 'Default')} |")
        lines.append(f"| Max Features | {rf_params.get('max_features', 'Default')} |")
        lines.append(f"| Bootstrap | {rf_params.get('bootstrap', 'True')} |")
        lines.append(f"| Random State | {rf_params.get('random_state', 'None')} |")
        
        return '\n'.join(lines)

    def _format_training_params(self, metadata: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–è®­ç»ƒå‚æ•°"""
        lines = []
        
        if 'cv_folds' in metadata:
            lines.append(f"| Cross-Validation Folds | {metadata['cv_folds']} |")
        if 'scoring_metric' in metadata:
            lines.append(f"| Scoring Metric | {metadata['scoring_metric']} |")
        if 'task_type' in metadata:
            lines.append(f"| Task Type | {metadata['task_type'].title()} |")
        
        return '\n'.join(lines)

    def _format_key_findings(self, results: Dict[str, Any], metadata: Dict[str, Any], has_hyperopt: bool) -> str:
        """æ ¼å¼åŒ–å…³é”®å‘ç°"""
        findings = []
        
        # æ¨¡å‹æ€§èƒ½
        if 'cv_scores' in results:
            cv_scores = results['cv_scores']
            if isinstance(cv_scores, dict) and 'test_score' in cv_scores:
                scores = cv_scores['test_score']
                mean_score = np.mean(scores)
                findings.append(f"1. **Model Performance**: The Random Forest achieved a cross-validation score of {mean_score:.6f}")
        
        # è¶…å‚æ•°ä¼˜åŒ–
        if has_hyperopt:
            findings.append("2. **Hyperparameter Optimization**: Systematic optimization improved model performance")
        
        # ç‰¹å¾é‡è¦æ€§
        n_features = metadata.get('n_features', 0)
        if n_features > 0:
            findings.append(f"3. **Feature Analysis**: {n_features} features analyzed for predictive importance")
        
        # ç¨³å®šæ€§
        if 'cv_scores' in results:
            cv_scores = results['cv_scores']
            if isinstance(cv_scores, dict) and 'test_score' in cv_scores:
                scores = cv_scores['test_score']
                std_score = np.std(scores)
                findings.append(f"4. **Model Stability**: Cross-validation standard deviation of {std_score:.6f} indicates consistent performance")
        
        return '\n'.join(findings) if findings else "1. **Training Completed**: Model training finished successfully"

    def _format_reproducibility_files(self, files: Dict[str, List[str]], model_directory: Path) -> str:
        """æ ¼å¼åŒ–å¯å¤ç°æ€§æ–‡ä»¶"""
        lines = []
        
        # æ¨¡å‹æ–‡ä»¶
        if 'model.pkl' in files.get('models', []):
            lines.append(f"- **Model Weights**: `{model_directory}/model.pkl`")
        
        # å…ƒæ•°æ®
        if 'model_metadata.json' in files.get('metadata', []):
            lines.append(f"- **Model Configuration**: `{model_directory}/model_metadata.json`")
        
        # é¢„å¤„ç†å™¨
        if 'preprocessor.pkl' in files.get('models', []):
            lines.append(f"- **Data Preprocessor**: `{model_directory}/preprocessor.pkl`")
        
        # äº¤å‰éªŒè¯æ•°æ®
        cv_data_files = files.get('cv_data', [])
        if cv_data_files:
            lines.append(f"- **Cross-Validation Data**: `{model_directory}/cross_validation_data/`")
        
        # ç‰¹å¾é‡è¦æ€§
        if 'feature_importance.csv' in files.get('data', []):
            lines.append(f"- **Feature Importance**: `{model_directory}/feature_importance.csv`")
        
        return '\n'.join(lines) if lines else "- **Model Directory**: Complete model artifacts saved"

    def _format_technical_implementation(self) -> str:
        """æ ¼å¼åŒ–æŠ€æœ¯å®ç°"""
        return """- **Framework**: scikit-learn for Random Forest implementation
- **Data Processing**: pandas and numpy for data handling
- **Cross-Validation**: K-fold cross-validation with stratification support
- **Feature Importance**: Built-in Random Forest feature importance calculation
- **Serialization**: Pickle for model and preprocessor persistence"""

    def _scan_model_files(self, model_directory: Path) -> Dict[str, List[str]]:
        """æ‰«ææ¨¡å‹ç›®å½•ä¸­çš„æ–‡ä»¶"""
        files = {
            'models': [],
            'data': [],
            'plots': [],
            'metadata': [],
            'cv_data': []
        }
        
        if not model_directory.exists():
            return files
        
        for file_path in model_directory.rglob('*'):
            if file_path.is_file():
                file_name = file_path.name
                
                # æ£€æŸ¥æ˜¯å¦åœ¨cross_validation_dataå­ç›®å½•ä¸­
                is_in_cv_data = 'cross_validation_data' in str(file_path.parent)
                
                # åˆ†ç±»æ–‡ä»¶
                if file_name.endswith(('.pkl', '.joblib')):
                    files['models'].append(file_name)
                elif file_name.endswith('.csv'):
                    if is_in_cv_data:
                        files['cv_data'].append(file_name)
                    else:
                        files['data'].append(file_name)
                elif file_name.endswith(('.png', '.jpg', '.jpeg', '.pdf', '.svg')):
                    if is_in_cv_data:
                        files['cv_data'].append(file_name)
                    else:
                        files['plots'].append(file_name)
                elif file_name.endswith('.json'):
                    files['metadata'].append(file_name)
        
        return files

    def _format_file_structure(self, files: Dict[str, List[str]], model_directory: Path) -> str:
        """æ ¼å¼åŒ–æ–‡ä»¶ç»“æ„"""
        structure_lines = [f"{model_directory.name}/"]
        
        # æ·»åŠ ä¸»è¦æ–‡ä»¶
        for category, file_list in files.items():
            if file_list:
                if category == 'cv_data':
                    structure_lines.append("â”œâ”€â”€ cross_validation_data/")
                    for file_name in sorted(file_list):
                        structure_lines.append(f"â”‚   â”œâ”€â”€ {file_name}")
                else:
                    for file_name in sorted(file_list):
                        structure_lines.append(f"â”œâ”€â”€ {file_name}")
        
        structure_lines.append("â””â”€â”€ academic_report.md               # This report")
        
        return '\n'.join(structure_lines)

    def _format_json_artifacts_section(self, files: Dict[str, List[str]], model_directory: Path) -> str:
        """æ ¼å¼åŒ–JSONæ–‡ä»¶éƒ¨åˆ†"""
        lines = []
        
        # äº¤å‰éªŒè¯æ•°æ®
        if 'cross_validation_data' in files.get('cv_data', []):
            lines.append(f"- **Cross-Validation Data**: `{model_directory}/cross_validation_data/`")
        
        # ç‰¹å¾é‡è¦æ€§
        if 'feature_importance.csv' in files.get('data', []):
            lines.append(f"- **Feature Importance**: `{model_directory}/feature_importance.csv`")
        
        return '\n'.join(lines) if lines else "- **Model Directory**: Complete model artifacts saved"

    def _format_data_validation_section(self, validation_results: Optional[Dict[str, Any]]) -> str:
        """Format comprehensive data validation section with detailed analysis, auto-loading from file if needed"""
        
        # å¦‚æœæ²¡æœ‰ç›´æ¥ä¼ å…¥validation_resultsï¼Œå°è¯•ä»æ–‡ä»¶åŠ è½½
        if not validation_results:
            validation_results = self._load_validation_results_from_file()
        
        if not validation_results:
            return """
### 2.4 Data Quality Assessment

**Status**: No comprehensive validation performed

Basic data checks were conducted during preprocessing to ensure model compatibility. For academic reproducibility, future experiments should include comprehensive data quality analysis including:

- Missing value assessment and handling strategies
- Outlier detection using statistical methods (IQR, Z-score)
- Feature correlation analysis (Pearson, Spearman, Kendall)
- Multicollinearity detection using Variance Inflation Factor (VIF)
- Feature distribution analysis (normality tests, skewness evaluation)
- Sample balance verification for classification tasks

**Recommendation**: Enable data validation (`validate_data=True`) in future training runs to ensure data quality standards for academic publication and experimental reproducibility.
"""
        
        # Extract validation details
        quality_assessment = validation_results.get('quality_assessment', {})
        overall_score = quality_assessment.get('overall_score', 'N/A')
        quality_level = quality_assessment.get('quality_level', 'Unknown')
        data_ready = validation_results.get('data_ready_for_training', False)
        
        # Count issues by severity
        validation_checks = validation_results.get('validation_results', [])
        critical_issues = []
        warnings = []
        
        for check in validation_checks:
            if not check.get('passed', True):
                critical_issues.extend(check.get('issues', []))
            else:
                warnings.extend(check.get('warnings', []))
        
        # Format validation results table
        validation_table = "| Check Name | Method Used | Status | Issues Found | Key Findings |\n|------------|-------------|--------|-------------|-------------|\n"
        
        for check in validation_checks:
            check_name = check.get('check_name', 'Unknown').replace('_', ' ').title()
            status = "âœ… PASSED" if check.get('passed', True) else "âŒ FAILED"
            issues_count = len(check.get('issues', []))
            method = self._get_validation_method(check.get('check_name', ''))
            key_findings = self._extract_key_findings(check)
            validation_table += f"| {check_name} | {method} | {status} | {issues_count} | {key_findings} |\n"
        
        # Generate detailed analysis for each validation type
        detailed_analysis = self._generate_detailed_validation_analysis_academic(validation_results)
        
        # Recommendations
        recommendations = validation_results.get('recommendations', [])
        recommendations_text = ""
        if recommendations:
            recommendations_text = "**Data Quality Recommendations:**\n\n"
            for i, rec in enumerate(recommendations[:12], 1):
                # Ensure rec is a string (handle any non-string types)
                if isinstance(rec, str):
                    recommendations_text += f"{i}. {rec}\n"
                else:
                    recommendations_text += f"{i}. {str(rec)}\n"
        
        # Statistical summary
        statistical_summary = self._generate_statistical_summary(validation_results)
        
        return f"""
### 2.4 Data Quality Assessment

Comprehensive data validation was performed using multiple statistical methods to ensure dataset quality and suitability for machine learning model training. The validation framework employed established statistical techniques for thorough data quality assessment.

#### 2.4.1 Overall Quality Metrics

| Metric | Value | Threshold | Interpretation |
|--------|-------|-----------|----------------|
| Overall Data Quality Score | {overall_score}/100 | â‰¥80 (Excellent), â‰¥60 (Good) | {self._interpret_quality_score(overall_score)} |
| Quality Level | {quality_level} | - | Categorical assessment |
| Ready for Training | {"Yes" if data_ready else "No"} | Yes | Model training readiness |
| Critical Issues | {len(critical_issues)} | 0 | Data integrity problems |
| Warnings | {len(warnings)} | <5 | Minor data quality concerns |

#### 2.4.2 Validation Methodology and Results

{validation_table}

{detailed_analysis}

{statistical_summary}

#### 2.4.3 Data Quality Issues and Impact Assessment

{"**Critical Issues Identified:**" if critical_issues else "**No critical issues detected.**"}

{chr(10).join([f"- {issue}" for issue in critical_issues[:12]]) if critical_issues else "All validation checks passed successfully, indicating high data quality suitable for academic research with strong statistical foundations."}

{recommendations_text}

#### 2.4.4 Academic and Methodological Implications

The data validation results indicate that the dataset {"meets" if data_ready else "does not meet"} the quality standards required for academic machine learning research. {self._get_academic_implications(overall_score, data_ready, len(critical_issues))}

**Reproducibility Impact**: {self._get_reproducibility_impact(validation_results)}
"""

    def _format_enhanced_feature_importance_section(self, 
                                                   feature_importance_data: Optional[Dict[str, Any]], 
                                                   files: Dict[str, List[str]], 
                                                   metadata: Dict[str, Any]) -> str:
        """Format comprehensive feature importance section with both tree-based and permutation methods"""
        
        lines = [
            "#### Feature Importance Analysis",
            "",
            "This analysis employs multiple methodologies to comprehensively evaluate feature importance in the Random Forest model:",
            ""
        ]
        
        # Method explanations
        lines.extend([
            "**Analysis Methods:**",
            "",
            "1. **Tree-based Importance (Mean Decrease in Impurity)**:",
            "   - Built-in Random Forest feature importance metric",
            "   - Measures average reduction in node impurity when feature is used for splits",
            "   - Fast computation, but can be biased toward high-cardinality features",
            "   - Formula: Average decrease in Gini impurity (classification) or MSE (regression)",
            "",
            "2. **Permutation Importance**:",
            "   - Model-agnostic method measuring feature contribution to model performance",
            "   - Evaluates performance drop when feature values are randomly shuffled",
            "   - More reliable for correlated features and unbiased feature ranking",
            "   - Computed on out-of-sample data to avoid overfitting",
            ""
        ])
        
        # Add actual analysis results if available
        if feature_importance_data:
            # Tree-based importance
            if 'basic_importance' in feature_importance_data:
                basic_importance = feature_importance_data['basic_importance']
                top_features = basic_importance.get('top_features', [])
                
                if top_features:
                    lines.extend([
                        "**Tree-based Feature Importance (Top 10):**",
                        "",
                        "| Rank | Feature | Importance | Percentage |",
                        "|------|---------|------------|------------|"
                    ])
                    
                    for i, feature_info in enumerate(top_features[:10], 1):
                        feature_name = feature_info.get('feature', f'Feature_{i}')
                        importance = feature_info.get('importance', 0)
                        percentage = feature_info.get('importance_percent', 0)
                        lines.append(f"| {i} | `{feature_name}` | {importance:.4f} | {percentage:.2f}% |")
                    
                    lines.append("")
            
            # Permutation importance
            if 'permutation_importance_results' in feature_importance_data:
                perm_importance = feature_importance_data['permutation_importance_results']
                perm_features = perm_importance.get('sorted_features', [])
                
                if perm_features:
                    lines.extend([
                        "**Permutation Importance (Top 10):**",
                        "",
                        "| Rank | Feature | Mean Importance | Std Dev | 95% CI |",
                        "|------|---------|-----------------|---------|--------|"
                    ])
                    
                    for i, feature_info in enumerate(perm_features[:10], 1):
                        feature_name = feature_info.get('feature', f'Feature_{i}')
                        mean_importance = feature_info.get('importance_mean', 0)
                        std_importance = feature_info.get('importance_std', 0)
                        
                        # Calculate 95% confidence interval
                        ci_lower = mean_importance - 1.96 * std_importance
                        ci_upper = mean_importance + 1.96 * std_importance
                        ci_text = f"[{ci_lower:.4f}, {ci_upper:.4f}]"
                        
                        lines.append(f"| {i} | `{feature_name}` | {mean_importance:.4f} | {std_importance:.4f} | {ci_text} |")
                    
                    lines.append("")
            
            # Method comparison insights
            if ('basic_importance' in feature_importance_data and 
                'permutation_importance_results' in feature_importance_data):
                
                lines.extend([
                    "**Method Comparison Insights:**",
                    "",
                    "- **Consistency Check**: Compare rankings between tree-based and permutation methods",
                    "- **Stability Analysis**: Permutation method provides uncertainty estimates via standard deviation",
                    "- **Bias Assessment**: Significant differences may indicate feature correlation effects",
                    "- **Model Reliability**: High agreement between methods suggests robust feature selection",
                    ""
                ])
        
        # Feature importance visualizations
        plot_files = files.get('plots', [])
        importance_plots = [f for f in plot_files if 'importance' in f.lower()]
        
        if importance_plots:
            lines.extend([
                "**Feature Importance Visualizations:**",
                ""
            ])
            
            for plot_file in importance_plots:
                if 'basic' in plot_file.lower() or 'tree' in plot_file.lower():
                    lines.extend([
                        f"![Tree-based Feature Importance]({plot_file})",
                        "",
                        f"**Tree-based Importance Plot**: `{plot_file}`",
                        ""
                    ])
                elif 'permutation' in plot_file.lower():
                    lines.extend([
                        f"![Permutation Feature Importance]({plot_file})",
                        "",
                        f"**Permutation Importance Plot**: `{plot_file}`",
                        ""
                    ])
                elif 'comparison' in plot_file.lower():
                    lines.extend([
                        f"![Feature Importance Comparison]({plot_file})",
                        "",
                        f"**Method Comparison Plot**: `{plot_file}`",
                        ""
                    ])
        
        # Feature importance data files
        data_files = files.get('data', [])
        importance_data_files = [f for f in data_files if 'importance' in f.lower()]
        
        if importance_data_files:
            lines.extend([
                "**Feature Importance Data Files:**",
                ""
            ])
            
            for data_file in importance_data_files:
                lines.append(f"- `{data_file}` - Detailed feature importance scores and statistics")
            
            lines.append("")
        
        # Statistical significance and interpretation
        lines.extend([
            "**Statistical Interpretation:**",
            "",
            "- **Threshold Selection**: Features with importance > 1/n_features are considered significant",
            "- **Cumulative Importance**: Top features typically capture 80-90% of total importance",
            "- **Stability Assessment**: Low standard deviation in permutation importance indicates reliable features",
            "- **Domain Validation**: Feature rankings should align with domain knowledge and expectations",
            ""
        ])
        
        # Technical notes
        lines.extend([
            "**Technical Implementation Notes:**",
            "",
            "- Tree-based importance computed using sklearn's `feature_importances_` attribute",
            "- Permutation importance calculated with 10 repetitions for statistical robustness",
            "- Random state fixed for reproducible permutation results",
            "- Analysis performed on validation data to avoid overfitting bias",
            ""
        ])
        
        return '\n'.join(lines)

    def _load_validation_results_from_file(self) -> Optional[Dict[str, Any]]:
        """Load validation results from data_validation_report.json file"""
        import json
        from pathlib import Path
        
        # å°è¯•ä»å½“å‰æ¨¡å‹ç›®å½•æˆ–è€…trained_modelsç›®å½•æ‰¾åˆ°éªŒè¯æŠ¥å‘Š
        possible_paths = [
            Path("reports/data_validation_report.json"),
            Path("data_validation_report.json"),
        ]
        
        # å¦‚æœæœ‰æ¨¡å‹ç›®å½•ï¼Œæ·»åŠ æ›´å¤šè·¯å¾„
        if hasattr(self, '_current_model_dir') and self._current_model_dir:
            model_dir = Path(self._current_model_dir)
            possible_paths.extend([
                model_dir / "reports" / "data_validation_report.json",
                model_dir / "data_validation_report.json"
            ])
        
        for path in possible_paths:
            if path.exists():
                try:
                    with open(path, 'r', encoding='utf-8') as f:
                        return json.load(f)
                except Exception as e:
                    logger.warning(f"Failed to load validation report from {path}: {e}")
                    continue
        
        return None
    
    def _get_validation_method(self, check_name: str) -> str:
        """Get the statistical method used for each validation check"""
        methods = {
            'missing_values': 'Completeness Analysis',
            'feature_correlations': 'Pearson/Spearman/Kendall',
            'multicollinearity_detection': 'Variance Inflation Factor (VIF)',
            'feature_distributions': 'Shapiro-Wilk, Jarque-Bera, D\'Agostino',
            'sample_balance': 'Chi-square, Gini coefficient',
            'data_types': 'Type Inference Analysis',
            'outlier_detection': 'IQR Method, Z-score',
            'duplicate_detection': 'Hash-based Comparison'
        }
        return methods.get(check_name, 'Statistical Analysis')
    
    def _extract_key_findings(self, check: Dict[str, Any]) -> str:
        """Extract key findings from validation check details"""
        check_name = check.get('check_name', '')
        details = check.get('details', {})
        
        if check_name == 'feature_correlations':
            high_corr = len(details.get('high_correlations', []))
            return f"{high_corr} high correlations"
        elif check_name == 'multicollinearity_detection':
            high_vif = len(details.get('high_vif_features', []))
            avg_vif = details.get('average_vif', 0)
            return f"{high_vif} high VIF; avg={avg_vif:.2f}"
        elif check_name == 'feature_distributions':
            issues = len(details.get('distribution_issues', []))
            return f"{issues} distribution issues"
        elif check_name == 'sample_balance':
            is_balanced = details.get('is_balanced', True)
            ratio = details.get('minority_class_ratio', 0)
            return f"{'Balanced' if is_balanced else 'Imbalanced'}; ratio={ratio:.3f}"
        else:
            issues = len(check.get('issues', []))
            return f"{issues} issues found" if issues > 0 else "No issues"
    
    def _generate_detailed_validation_analysis_academic(self, validation_results: Dict[str, Any]) -> str:
        """Generate detailed academic analysis for each validation type"""
        analysis = []
        
        for check in validation_results.get('validation_results', []):
            check_name = check.get('check_name', '')
            details = check.get('details', {})
            
            # Ensure details is a dictionary
            if not isinstance(details, dict):
                continue
            
            if check_name == 'feature_correlations':
                analysis.append(self._format_correlation_analysis_academic(details))
            elif check_name == 'multicollinearity_detection':
                analysis.append(self._format_multicollinearity_analysis_academic(details))
            elif check_name == 'feature_distributions':
                analysis.append(self._format_distribution_analysis_academic(details))
            elif check_name == 'sample_balance':
                analysis.append(self._format_balance_analysis_academic(details))
        
        return '\n\n'.join(filter(None, analysis))
    
    def _format_correlation_analysis_academic(self, details: Dict[str, Any]) -> str:
        """Format correlation analysis for academic report"""
        if not details or not isinstance(details, dict):
            return ""
        
        high_corr = details.get('high_correlations', [])
        threshold = details.get('correlation_threshold', 0.8)
        
        return f"""#### 2.4.2.1 Feature Correlation Analysis

**Methodology**: Pearson, Spearman, and Kendall correlation coefficients were computed for all feature pairs. The correlation threshold was set at |r| â‰¥ {threshold}.

**Results**: {len(high_corr)} feature pairs exceeded the correlation threshold, indicating potential redundancy in the feature space.

**Statistical Findings**:
{self._format_correlation_table_academic(high_corr[:8]) if high_corr else "- No highly correlated features detected (|r| < " + str(threshold) + ")"}

**Impact Assessment**: {"High feature correlation may lead to multicollinearity issues and reduced model interpretability." if high_corr else "Low feature correlation indicates good feature independence, supporting model stability."}"""
    
    def _format_correlation_table_academic(self, correlations: List[Dict]) -> str:
        """Format correlation table for academic report"""
        if not correlations:
            return ""
        
        table = "| Feature Pair | Correlation Type | Coefficient | Significance |\n|--------------|------------------|-------------|-------------|\n"
        for corr in correlations:
            f1, f2 = corr.get('feature1', 'N/A'), corr.get('feature2', 'N/A')
            corr_type = corr.get('correlation_type', 'N/A')
            coeff = corr.get('max_correlation', 0)
            significance = "High" if abs(coeff) >= 0.9 else "Moderate" if abs(coeff) >= 0.7 else "Low"
            table += f"| {f1} - {f2} | {corr_type} | {coeff:.3f} | {significance} |\n"
        
        return table
    
    def _format_multicollinearity_analysis_academic(self, details: Dict[str, Any]) -> str:
        """Format multicollinearity analysis for academic report"""
        if not details or not isinstance(details, dict):
            return ""
        
        vif_threshold = details.get('vif_threshold', 5.0)
        high_vif = details.get('high_vif_features', [])
        avg_vif = details.get('average_vif', 0)
        max_vif = details.get('max_vif', 0)
        
        return f"""#### 2.4.2.2 Multicollinearity Detection

**Methodology**: Variance Inflation Factor (VIF) analysis was conducted using linear regression. VIF values â‰¥ {vif_threshold} indicate problematic multicollinearity.

**Results**: 
- Average VIF: {avg_vif:.3f}
- Maximum VIF: {max_vif:.3f}
- Features with VIF â‰¥ {vif_threshold}: {len(high_vif)}

**Statistical Findings**:
{self._format_vif_table_academic(high_vif[:6]) if high_vif else f"- All features showed VIF < {vif_threshold}, indicating acceptable multicollinearity levels"}

**Methodological Impact**: {"Elevated VIF scores suggest linear dependencies between predictors, which may compromise model stability and coefficient interpretation." if high_vif else "Low VIF scores support the assumption of feature independence required for many machine learning algorithms."}"""
    
    def _format_vif_table_academic(self, vif_features: List[Dict]) -> str:
        """Format VIF table for academic report"""
        if not vif_features:
            return ""
        
        table = "| Feature | VIF Score | RÂ² | Interpretation |\n|---------|-----------|----|--------------|\n"
        for feature in vif_features:
            name = feature.get('feature', 'N/A')
            vif = feature.get('vif_score', 0)
            r2 = feature.get('r_squared', 0)
            interpretation = "Severe" if vif >= 10 else "Moderate" if vif >= 5 else "Acceptable"
            table += f"| {name} | {vif:.3f} | {r2:.3f} | {interpretation} |\n"
        
        return table
    
    def _format_distribution_analysis_academic(self, details: Dict[str, Any]) -> str:
        """Format distribution analysis for academic report"""
        if not details or not isinstance(details, dict):
            return ""
        
        cont_dist = details.get('continuous_distributions', {})
        cat_dist = details.get('categorical_distributions', {})
        issues = details.get('distribution_issues', [])
        
        return f"""#### 2.4.2.3 Feature Distribution Analysis

**Methodology**: 
- Continuous features: Shapiro-Wilk test (nâ‰¤5000), Jarque-Bera test (nâ‰¥50), D'Agostino test (nâ‰¥20) for normality
- Skewness assessment using sample skewness coefficient
- Outlier detection via Interquartile Range (IQR) method
- Categorical features: Gini coefficient, entropy, and class imbalance ratio analysis

**Results**: {len(issues)} distribution-related issues identified across {len(cont_dist)} continuous and {len(cat_dist)} categorical features.

**Continuous Features Statistical Summary**:
{self._format_continuous_features_academic(cont_dist)}

**Categorical Features Statistical Summary**:
{self._format_categorical_features_academic(cat_dist)}

**Distribution Quality Impact**: {"Significant distribution anomalies may affect model performance and require preprocessing interventions." if issues else "Feature distributions meet statistical assumptions for machine learning applications."}"""
    
    def _format_continuous_features_academic(self, cont_dist: Dict[str, Any]) -> str:
        """Format continuous features analysis for academic report"""
        if not cont_dist:
            return "No continuous features analyzed."
        
        table = "| Feature | Skewness | Normality | Outliers (%) | Issues |\n|---------|----------|-----------|-------------|--------|\n"
        
        for feature, data in list(cont_dist.items())[:10]:
            # Ensure data is a dictionary
            if not isinstance(data, dict):
                continue
                
            skewness = data.get('skewness_analysis', {}).get('skewness', 0)
            is_normal = data.get('normality_tests', {}).get('is_normal', False)
            outlier_ratio = data.get('outlier_detection', {}).get('outlier_ratio', 0) * 100
            issues = len(data.get('identified_issues', []))
            
            normality_text = "Yes" if is_normal else "No"
            table += f"| {feature} | {skewness:.3f} | {normality_text} | {outlier_ratio:.1f}% | {issues} |\n"
        
        return table
    
    def _format_categorical_features_academic(self, cat_dist: Dict[str, Any]) -> str:
        """Format categorical features analysis for academic report"""
        if not cat_dist:
            return "No categorical features analyzed."
        
        table = "| Feature | Classes | Gini Coeff | Imbalance Ratio | Entropy | Issues |\n|---------|---------|------------|-----------------|---------|--------|\n"
        
        for feature, data in list(cat_dist.items())[:10]:
            # Ensure data is a dictionary
            if not isinstance(data, dict):
                continue
                
            num_classes = data.get('basic_stats', {}).get('unique_count', 0)
            gini = data.get('imbalance_analysis', {}).get('gini_coefficient', 0)
            imbalance_ratio = data.get('imbalance_analysis', {}).get('imbalance_ratio', 1)
            entropy = data.get('imbalance_analysis', {}).get('entropy', 0)
            issues = len(data.get('identified_issues', []))
            
            table += f"| {feature} | {num_classes} | {gini:.3f} | {imbalance_ratio:.1f}:1 | {entropy:.3f} | {issues} |\n"
        
        return table
    
    def _format_balance_analysis_academic(self, details: Dict[str, Any]) -> str:
        """Format balance analysis for academic report"""
        if not details or not isinstance(details, dict):
            return ""
        
        minority_ratio = details.get('minority_class_ratio', 0)
        is_balanced = details.get('is_balanced', True)
        target_dist = details.get('target_distribution', {})
        
        return f"""#### 2.4.2.4 Sample Balance Analysis

**Methodology**: Chi-square goodness-of-fit test and Gini coefficient calculation for class distribution assessment.

**Results**:
- Minority class ratio: {minority_ratio:.4f}
- Dataset balance: {"Balanced" if is_balanced else "Imbalanced"}
- Number of classes: {len(target_dist)}

**Class Distribution**:
{self._format_class_distribution_academic(target_dist)}

**Methodological Implications**: {"Balanced class distribution supports unbiased model training and reliable performance metrics." if is_balanced else "Class imbalance may require sampling techniques or specialized evaluation metrics for valid model assessment."}"""
    
    def _format_class_distribution_academic(self, target_dist: Dict) -> str:
        """Format class distribution table for academic report"""
        if not target_dist:
            return "No class distribution data available."
        
        table = "| Class | Count | Proportion | Cumulative % |\n|-------|-------|------------|-------------|\n"
        total = sum(target_dist.values())
        cumulative = 0
        
        for class_val, count in sorted(target_dist.items(), key=lambda x: x[1], reverse=True):
            proportion = count / total
            cumulative += proportion
            table += f"| {class_val} | {count} | {proportion:.4f} | {cumulative*100:.1f}% |\n"
        
        return table
    
    def _generate_statistical_summary(self, validation_results: Dict[str, Any]) -> str:
        """Generate statistical summary of validation results"""
        total_checks = len(validation_results.get('validation_results', []))
        passed_checks = sum(1 for check in validation_results.get('validation_results', []) if check.get('passed', True))
        
        return f"""#### 2.4.2.5 Statistical Summary

**Validation Framework Performance**:
- Total validation checks: {total_checks}
- Passed checks: {passed_checks} ({passed_checks/max(total_checks,1)*100:.1f}%)
- Failed checks: {total_checks - passed_checks}

**Data Quality Confidence**: Based on the comprehensive validation framework, the dataset demonstrates {"high" if passed_checks/max(total_checks,1) >= 0.8 else "moderate" if passed_checks/max(total_checks,1) >= 0.6 else "low"} statistical reliability for machine learning applications."""
    
    def _interpret_quality_score(self, score) -> str:
        """Interpret quality score for academic context"""
        if score == 'N/A':
            return "Not assessed"
        
        try:
            score = float(score)
            if score >= 90:
                return "Excellent - Suitable for publication"
            elif score >= 80:
                return "Good - Meets academic standards"
            elif score >= 70:
                return "Acceptable - Minor improvements needed"
            elif score >= 60:
                return "Fair - Requires improvement"
            else:
                return "Poor - Significant issues require resolution"
        except (ValueError, TypeError):
            return "Invalid score format"
    
    def _get_academic_implications(self, score, data_ready: bool, critical_issues: int) -> str:
        """Generate academic implications text"""
        if score == 'N/A':
            return "Data quality assessment was not performed, limiting confidence in experimental validity."
        
        try:
            score = float(score)
            if score >= 80 and data_ready and critical_issues == 0:
                return "High data quality supports robust experimental conclusions and enhances reproducibility of results. The dataset meets standards for academic publication."
            elif score >= 60:
                return "Moderate data quality with some limitations. Results should be interpreted with consideration of identified data quality issues."
            else:
                return "Poor data quality may compromise experimental validity. Significant preprocessing and quality improvements are recommended before publication."
        except (ValueError, TypeError):
            return "Data quality assessment results are inconclusive due to invalid score format."
    
    def _get_reproducibility_impact(self, validation_results: Dict[str, Any]) -> str:
        """Generate reproducibility impact assessment"""
        quality_score = validation_results.get('quality_assessment', {}).get('overall_score', 0)
        data_ready = validation_results.get('data_ready_for_training', False)
        
        if quality_score == 'N/A':
            return "Reproducibility assessment limited due to lack of data validation."
        
        try:
            score = float(quality_score)
            if score >= 80 and data_ready:
                return "High reproducibility confidence with comprehensive data quality documentation supporting experimental replication."
            elif score >= 60:
                return "Moderate reproducibility confidence. Additional data quality measures may improve experimental replication success."
            else:
                return "Low reproducibility confidence due to data quality issues. Preprocessing standardization required for reliable replication."
        except (ValueError, TypeError):
            return "Reproducibility assessment limited due to invalid quality score format."

    def generate_academic_report(self, model_directory: Union[str, Path]) -> Optional[str]:
        """
        Generate academic report from model directory (simplified interface).
        
        Args:
            model_directory: Path to model directory containing metadata and results
            
        Returns:
            Path to generated academic report or None if failed
        """
        try:
            model_directory = Path(model_directory)
            
            # Set current model directory for auto-loading validation reports
            self._current_model_dir = model_directory
            
            # Load metadata
            metadata_path = model_directory / "metadata.json"
            if not metadata_path.exists():
                logger.warning(f"Metadata file not found: {metadata_path}")
                return None
            
            with open(metadata_path, 'r') as f:
                metadata = json.load(f)
            
            # Load cross-validation results if available
            cv_results_path = model_directory / "cross_validation_results.json"
            training_results = {}
            if cv_results_path.exists():
                with open(cv_results_path, 'r') as f:
                    cv_results = json.load(f)
                    training_results = cv_results
            
            # Extract hyperparameter optimization results from metadata
            hyperopt_results = metadata.get('optimization_results')
            
            # Generate the report
            report_path = self.generate_report(
                model_directory=model_directory,
                model_metadata=metadata,
                training_results=training_results,
                hyperopt_results=hyperopt_results
            )
            
            return report_path
            
        except Exception as e:
            logger.error(f"Failed to generate academic report: {e}")
            return None
        finally:
            # Clean up
            if hasattr(self, '_current_model_dir'):
                delattr(self, '_current_model_dir') 

    # === Prediction Report Generation ===
    
    def generate_prediction_experiment_report_from_folder(self, prediction_folder: Union[str, Path],model_info:Dict[str,Any]=None) -> Optional[str]:
        """Generate markdown prediction experiment report from prediction folder containing saved files.
        
        Args:
            prediction_folder: Path to prediction folder containing results
            
        Returns:
            Path to generated markdown report or None if failed
        """
        try:
            prediction_folder = Path(prediction_folder)
            
            # Load prediction results from metadata
            metadata_path = prediction_folder / "prediction_metadata.json"
            if not metadata_path.exists():
                logger.error(f"Prediction metadata not found: {metadata_path}")
                return None
            
            with open(metadata_path, 'r', encoding='utf-8') as f:
                results = json.load(f)
            
            # Load input data
            input_data_path = prediction_folder / "01_original_data.csv"
            if not input_data_path.exists():
                logger.error(f"Original data not found: {input_data_path}")
                return None
            
            input_data = pd.read_csv(input_data_path)
            
            # Generate markdown report
            report_path = self._create_prediction_markdown_experiment_report(results, input_data, prediction_folder,model_info)
            
            logger.info(f"Markdown prediction experiment report generated: {report_path}")
            return str(report_path)
            
        except Exception as e:
            logger.error(f"Failed to generate markdown prediction experiment report: {e}")
            return None

    def _create_prediction_markdown_experiment_report(self, results: Dict[str, Any], input_data: pd.DataFrame, prediction_folder: Optional[Path] = None, model_info: Dict[str,Any] = None) -> str:
        """Create detailed Markdown experiment report."""
        from datetime import datetime
        
        # Extract key information with safe access
        model_id = results.get('model_id', 'Unknown')
        prediction_id = results.get('prediction_id', 'unknown_prediction')
        task_type = results.get('task_type', 'unknown')
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        # Get prediction metadata safely
        metadata = results.get('prediction_metadata', {})
        num_predictions = metadata.get('num_samples', len(input_data))
        data_shape = metadata.get('data_shape', 'N/A')
        processing_time = metadata.get('prediction_time_seconds', 0)
        preprocessing_applied = metadata.get('preprocessing_applied', False)
        
        # Get feature and target information
        feature_names = input_data.columns.tolist()
        num_features = len(feature_names)
        
        # Get real target names from model metadata
        model_metadata = results.get('model_metadata', {})
        if not model_metadata and model_info:
            model_metadata = model_info
            
        real_target_names = model_metadata.get('target_name', [])
        if not real_target_names:
            real_target_names = model_metadata.get('target_column', [])
        
        # Determine target information from predictions (safe access)
        predictions = results.get('predictions', [])
        
        # Handle both single predictions (float/int) and batch predictions (list)
        if isinstance(predictions, (list, np.ndarray)) and len(predictions) > 0:
            # Batch predictions
            sample_pred = predictions[0]
            if isinstance(sample_pred, (list, np.ndarray)):
                num_targets = len(sample_pred)
                # Use real target names if available, otherwise fallback to generic names
                if real_target_names and len(real_target_names) == num_targets:
                    target_names = real_target_names
                else:
                    target_names = [f"target_{i+1}" for i in range(num_targets)]
            else:
                num_targets = 1
                target_names = real_target_names[:1] if real_target_names else ["target"]
        elif isinstance(predictions, (int, float, np.number)):
            # Single prediction
            num_targets = 1
            target_names = real_target_names[:1] if real_target_names else ["target"]
            # Convert single prediction to list for consistency in downstream processing
            predictions = [predictions]
        else:
            # Default case
            num_targets = 1
            target_names = real_target_names[:1] if real_target_names else ["target"]
            if not isinstance(predictions, (list, np.ndarray)):
                predictions = [predictions] if predictions is not None else []
        
        # Calculate prediction statistics
        pred_stats = self._calculate_prediction_statistics(predictions, task_type, target_names)
        
        # Start building the markdown report
        md_content = f"""# Prediction Experiment Report

**Generated on:** {timestamp}  
**Experiment Name:** {prediction_id}  
**Model ID:** `{model_id}`  
**Output Directory:** `trained_models/{model_id}/predictions/{prediction_id}`

## Executive Summary

This report documents a comprehensive machine learning prediction experiment conducted using a pre-trained {task_type} model. The experiment involved preprocessing input data, making predictions, and providing detailed statistical analysis of the prediction results.

### Key Results
- **Number of Predictions:** {num_predictions:,}
- **Feature Count:** {num_features}
- **Target Count:** {num_targets}
- **Model Type:** {task_type.title()}
- **Processing Time:** {processing_time:.3f} seconds

---

## 1. Experiment Setup

### 1.1 Input Data Information

| Parameter | Value |
|-----------|-------|
| Number of Samples | {num_predictions:,} |
| Number of Features | {num_features} |
| Number of Targets | {num_targets} |
| Data Shape | {data_shape} |
| Data Type | Numerical (floating-point) |
| Preprocessing Applied | {'Yes' if preprocessing_applied else 'No'} |

### 1.2 Feature Information

**Input Features ({num_features} columns):**
{self._format_feature_list(feature_names)}

**Target Variables ({num_targets} column{'s' if num_targets > 1 else ''}):**
{self._format_feature_list(target_names)}

### 1.3 Model Information

| Component | Details |
|-----------|---------|
| **Model Type** | {task_type.title()} Model |
| **Model ID** | `{model_id}` |"""
        
        # Add model name if available
        if model_metadata.get('name'):
            md_content += f"\n| **Model Name** | {model_metadata['name']} |"
            
        md_content += f"""
| **Framework** | Random Forest (scikit-learn) |
| **Prediction Method** | {'Ensemble averaging' if task_type == 'regression' else 'Majority voting'} |

---

## 2. Prediction Results

### 2.1 Prediction Statistics

{self._format_prediction_statistics_md(pred_stats, task_type, target_names)}

---

## 3. Generated Files

| File | Description |
|------|-------------|"""
        
        # Add information about saved files
        if results.get('saved_files') and prediction_folder:
            file_descriptions = {
                'original_data': 'Original input data as provided',
                'processed_features': 'Features after preprocessing/normalization',
                'predictions_processed': 'Predictions in processed/normalized scale',
                'predictions_original': 'Predictions transformed back to original scale',
                'confidence_scores': 'Confidence/uncertainty scores for each prediction',
                'combined_results': 'All data combined: features + predictions + confidence',
                'metadata': 'Complete prediction metadata and configuration',
                'html_report': 'Interactive HTML report with detailed analysis'
            }
            
            # List the files that were actually saved
            saved_files = results.get('saved_files', {})
            for file_key, file_path in saved_files.items():
                file_name = Path(file_path).name
                description = file_descriptions.get(file_key, 'Prediction data file')
                md_content += f"\n| `{file_name}` | {description} |"
            
            # Add standard files
            md_content += f"\n| `prediction_experiment_report.md` | This detailed experiment report |"
            md_content += f"\n| `prediction_report.html` | Interactive HTML report |"
        else:
            md_content += f"\n| `prediction_results_{prediction_id}.csv` | Complete prediction results |"
            md_content += f"\n| `prediction_experiment_report.md` | This detailed experiment report |"
        
        # Add feature importance section if available
        if results.get('feature_importance'):
            md_content += "\n\n---\n\n## 4. Feature Importance Analysis\n\n"
            md_content += "| Feature | Importance | Relative Contribution |\n"
            md_content += "|---------|------------|----------------------|\n"
            
            feature_importance = results['feature_importance']
            total_importance = sum(feature_importance.values()) if feature_importance.values() else 1
            
            for feature, importance in sorted(feature_importance.items(), key=lambda x: x[1], reverse=True):
                relative_pct = (importance / total_importance) * 100 if total_importance > 0 else 0
                md_content += f"| {feature} | {importance:.4f} | {relative_pct:.1f}% |\n"
        
        # Add detailed prediction results section
        md_content += "\n\n---\n\n## 4. Detailed Prediction Results\n\n" if not results.get('feature_importance') else "\n\n---\n\n## 5. Detailed Prediction Results\n\n"
        md_content += "This section provides a comprehensive view of each prediction with corresponding input features and confidence scores.\n\n"
        
        # Create a simple prediction table - use raw predictions for label mapping
        raw_predictions = results.get('raw_predictions', predictions)
        predictions_list = raw_predictions if isinstance(raw_predictions, (list, np.ndarray)) else [raw_predictions]
        confidence_scores = results.get('confidence_scores', [])
        
        # Show up to 10 samples with key features
        max_samples_to_show = min(10, len(predictions_list))
        feature_cols = feature_names[:4]  # Show first 4 features for readability
        
        md_content += "### Feature Values and Predictions\n\n"
        
        # Create table header
        md_content += "| Sample | "
        for fname in feature_cols:
            md_content += f"{fname} | "
        if len(feature_names) > 4:
            md_content += "... | "
        prediction_label = "Prediction" + ("s" if num_targets > 1 else "")
        md_content += f"{prediction_label} | Confidence |\n"
        
        # Create separator
        md_content += "|--------|"
        for _ in feature_cols:
            md_content += "--------|"
        if len(feature_names) > 4:
            md_content += "-----|"
        md_content += "-----------|----------|\n"
        
        # Add data rows
        for i in range(max_samples_to_show):
            pred = predictions_list[i] if i < len(predictions_list) else 0
            conf = confidence_scores[i] if i < len(confidence_scores) else 0.0
            
            md_content += f"| **#{i+1}** |"
            
            # Add feature values
            for fname in feature_cols:
                if fname in input_data.columns and i < len(input_data):
                    feature_val = input_data.iloc[i][fname]
                    if isinstance(feature_val, (int, float, np.number)) and not isinstance(feature_val, bool):
                        md_content += f" {feature_val:.3f} |"
                    else:
                        md_content += f" {feature_val} |"
                else:
                    md_content += " - |"
            
            if len(feature_names) > 4:
                md_content += " ... |"
            
            # Add prediction - handle classification labels
            if task_type == 'classification' and results.get('label_mapping'):
                class_to_label = results['label_mapping'].get('class_to_label', {})
                
                def safe_academic_label_lookup(pred_value):
                    """Safely lookup classification label for academic report, handling both string and numeric predictions."""
                    if isinstance(pred_value, str):
                        return pred_value
                    try:
                        return (class_to_label.get(str(int(pred_value))) or 
                                class_to_label.get(int(pred_value)) or 
                                f'Class_{int(pred_value)}')
                    except (ValueError, TypeError):
                        return str(pred_value) if pred_value is not None else 'Unknown'
                
                if isinstance(pred, (list, np.ndarray)):
                    # Multi-target classification
                    pred_labels = [safe_academic_label_lookup(p) for p in pred]
                    pred_str = ", ".join(pred_labels)
                else:
                    # Single classification
                    pred_str = safe_academic_label_lookup(pred)
            else:
                # Regression or no label mapping
                if isinstance(pred, (list, np.ndarray)):
                    # Safely format each prediction - handle both numeric and string values
                    formatted_preds = []
                    for p in pred:
                        try:
                            formatted_preds.append(f"{float(p):.4f}")
                        except (ValueError, TypeError):
                            formatted_preds.append(str(p))
                    pred_str = ", ".join(formatted_preds)
                else:
                    # Safely format prediction - handle both numeric and string values
                    try:
                        pred_str = f"{float(pred):.4f}"
                    except (ValueError, TypeError):
                        pred_str = str(pred)
            md_content += f" {pred_str} |"
            
            # Add confidence
            if isinstance(conf, (list, np.ndarray)):
                conf_str = f"{conf[0]:.3f}" if len(conf) > 0 else "N/A"
            else:
                conf_str = f"{conf:.3f}"
            md_content += f" {conf_str} |\n"
        
        if len(predictions_list) > max_samples_to_show:
            md_content += f"\n*Showing {max_samples_to_show} of {len(predictions_list)} total predictions.*\n"
        
        md_content += "\n### Interpretation Guide\n\n"
        if task_type == 'regression':
            md_content += "- **Prediction Values**: Continuous numerical predictions representing the target variable\n"
        else:
            md_content += "- **Prediction Values**: Class labels or probabilities for classification\n"
        
        md_content += "- **Confidence Scores**: Range from 0.0 (low confidence) to 1.0 (high confidence)\n"
        md_content += "  - **â‰¥0.8**: High confidence - Very reliable predictions\n"
        md_content += "  - **0.5-0.8**: Medium confidence - Moderately reliable\n"
        md_content += "  - **<0.5**: Low confidence - Review recommended\n"
        
        # Add confidence analysis if available
        if results.get('confidence_scores'):
            next_section_num = 6 if results.get('feature_importance') else 5
            md_content += f"\n\n---\n\n## {next_section_num}. Confidence Analysis\n\n"
            
            # Add confidence calculation methodology
            md_content += f"### {next_section_num}.1 Confidence Calculation Method\n\n"
            
            if task_type == 'classification':
                md_content += "**Classification Confidence Calculation:**\n"
                md_content += "- **Method**: Maximum class probability from model predictions\n"
                md_content += "- **Formula**: `confidence = max(class_probabilities)`\n"
                md_content += "- **Range**: 0-1, where values closer to 1 indicate high certainty in the predicted class\n"
                md_content += "- **Interpretation**: Higher values mean the model is more confident about the predicted class\n\n"
            else:
                md_content += "**Regression Confidence Calculation:**\n"
                md_content += "- **Primary Method**: Prediction variance across individual trees in Random Forest ensemble\n"
                md_content += "- **Formula**: `confidence = 1 / (1 + variance_across_trees)`\n"
                md_content += "- **Range**: 0-1, where higher values indicate more confident predictions\n"
                md_content += "- **Multi-target Handling**: For models with multiple targets, confidence is averaged across all targets\n"
                md_content += "- **Fallback Method**: For non-ensemble models, uses inverse relationship with prediction magnitude\n\n"
                
                md_content += "**Technical Details:**\n"
                md_content += "- Each tree in the Random Forest makes an independent prediction\n"
                md_content += "- Variance across these individual predictions indicates uncertainty\n"
                md_content += "- Low variance (trees agree) â†’ High confidence\n"
                md_content += "- High variance (trees disagree) â†’ Low confidence\n\n"
            
            md_content += f"### {next_section_num}.2 Confidence Statistics\n\n"
            confidence_stats = self._analyze_confidence_scores(results['confidence_scores'])
            md_content += f"**Mean Confidence:** {confidence_stats['mean']:.3f}  \n"
            md_content += f"**Standard Deviation:** {confidence_stats['std']:.3f}  \n"
            md_content += f"**Min Confidence:** {confidence_stats['min']:.3f}  \n"
            md_content += f"**Max Confidence:** {confidence_stats['max']:.3f}  \n\n"
            
            md_content += f"### {next_section_num}.3 Confidence Distribution\n\n"
            md_content += "| Confidence Level | Count | Percentage | Description |\n"
            md_content += "|------------------|-------|------------|-------------|\n"
            md_content += f"| High (â‰¥0.8) | {confidence_stats['high_count']} | {confidence_stats['high_pct']:.1f}% | Very reliable predictions |\n"
            md_content += f"| Medium (0.5-0.8) | {confidence_stats['medium_count']} | {confidence_stats['medium_pct']:.1f}% | Moderately reliable predictions |\n"
            md_content += f"| Low (<0.5) | {confidence_stats['low_count']} | {confidence_stats['low_pct']:.1f}% | Uncertain predictions - review recommended |\n\n"
            
            md_content += f"### {next_section_num}.4 Confidence Interpretation Guide\n\n"
            if task_type == 'classification':
                md_content += "**For Classification Models:**\n"
                md_content += "- **High Confidence (â‰¥0.8)**: Strong certainty in predicted class\n"
                md_content += "- **Medium Confidence (0.5-0.8)**: Moderate certainty, acceptable for most applications\n"
                md_content += "- **Low Confidence (<0.5)**: Uncertain prediction, consider:\n"
                md_content += "  - Reviewing input features for anomalies\n"
                md_content += "  - Gathering more training data for similar cases\n"
                md_content += "  - Using ensemble of multiple models\n\n"
            else:
                md_content += "**For Regression Models:**\n"
                md_content += "- **High Confidence (â‰¥0.8)**: Trees in ensemble show strong agreement\n"
                md_content += "- **Medium Confidence (0.5-0.8)**: Moderate agreement, reliable for most applications\n"
                md_content += "- **Low Confidence (<0.5)**: High prediction variance, may indicate:\n"
                md_content += "  - Input data outside training distribution\n"
                md_content += "  - Insufficient training data for similar cases\n"
                md_content += "  - High inherent noise in target variable\n"
                md_content += "  - Model complexity mismatch with data complexity\n\n"
        
        # Add footer
        md_content += f"\n\n---\n\n*Report generated on {timestamp}*\n"
        md_content += f"*MCP Random Forest Tool - Prediction Experiment Report*\n"
        
        # Save markdown report to file if prediction folder is provided
        if prediction_folder:
            try:
                report_path = prediction_folder / "prediction_experiment_report.md"
                with open(report_path, 'w', encoding='utf-8') as f:
                    f.write(md_content)
                logger.info(f"Markdown experiment report saved to: {report_path}")
                return str(report_path)
            except Exception as e:
                logger.error(f"Failed to save markdown report: {e}")
                return md_content
        else:
            return md_content

    def _format_feature_list(self, features: List[str]) -> str:
        """Format feature list for markdown."""
        if len(features) <= 10:
            return "`" + "`, `".join(features) + "`"
        else:
            first_five = features[:5]
            last_two = features[-2:]
            return "`" + "`, `".join(first_five) + "`, ..., `" + "`, `".join(last_two) + "`"
    
    def _calculate_prediction_statistics(self, predictions: List, task_type: str, target_names: Optional[List[str]] = None) -> Dict[str, Any]:
        """Calculate comprehensive prediction statistics."""
        if not predictions:
            return {}
        
        predictions_array = np.array(predictions)
        
        if task_type == 'regression':
            if predictions_array.ndim == 1:
                # Single target regression
                return {
                    'mean': float(np.mean(predictions_array)),
                    'std': float(np.std(predictions_array)),
                    'min': float(np.min(predictions_array)),
                    'max': float(np.max(predictions_array)),
                    'range': float(np.max(predictions_array) - np.min(predictions_array)),
                    'median': float(np.median(predictions_array)),
                    'q25': float(np.percentile(predictions_array, 25)),
                    'q75': float(np.percentile(predictions_array, 75))
                }
            else:
                # Multi-target regression
                stats = {}
                n_targets = predictions_array.shape[1]
                for i in range(n_targets):
                    target_preds = predictions_array[:, i]
                    # Use real target name if available, otherwise fallback to generic name
                    if target_names and len(target_names) == n_targets:
                        target_key = target_names[i]
                    else:
                        target_key = f'target_{i+1}'
                    
                    stats[target_key] = {
                        'mean': float(np.mean(target_preds)),
                        'std': float(np.std(target_preds)),
                        'min': float(np.min(target_preds)),
                        'max': float(np.max(target_preds)),
                        'range': float(np.max(target_preds) - np.min(target_preds)),
                        'median': float(np.median(target_preds)),
                        'q25': float(np.percentile(target_preds, 25)),
                        'q75': float(np.percentile(target_preds, 75))
                    }
                return stats
        else:
            # Classification statistics
            from collections import Counter
            class_counts = Counter(predictions)
            total = len(predictions)
            return {
                'class_distribution': dict(class_counts),
                'unique_classes': len(class_counts),
                'most_common_class': class_counts.most_common(1)[0] if class_counts else None,
                'total_predictions': total
            }
    
    def _format_prediction_statistics_md(self, stats: Dict[str, Any], task_type: str, target_names: List[str]) -> str:
        """Format prediction statistics for markdown."""
        if not stats:
            return "No prediction statistics available."
        
        if task_type == 'regression':
            if len(target_names) == 1:
                # Single target regression
                return f"""
#### Single Target Prediction Statistics

**Target: {target_names[0]}**

| Statistic | Value |
|-----------|-------|
| Mean Prediction | {stats['mean']:.6f} |
| Standard Deviation | {stats['std']:.6f} |
| Minimum Prediction | {stats['min']:.6f} |
| Maximum Prediction | {stats['max']:.6f} |
| Prediction Range | {stats['range']:.6f} |
| Median | {stats['median']:.6f} |
| 25th Percentile | {stats['q25']:.6f} |
| 75th Percentile | {stats['q75']:.6f} |
"""
            else:
                # Multi-target regression
                md = "\n#### Multi-Target Prediction Statistics\n\n"
                for target_name in target_names:
                    if target_name in stats:
                        target_stats = stats[target_name]
                        md += f"**Target: {target_name}**\n\n"
                        md += "| Statistic | Value |\n"
                        md += "|-----------|-------|\n"
                        md += f"| Mean Prediction | {target_stats['mean']:.6f} |\n"
                        md += f"| Standard Deviation | {target_stats['std']:.6f} |\n"
                        md += f"| Minimum Prediction | {target_stats['min']:.6f} |\n"
                        md += f"| Maximum Prediction | {target_stats['max']:.6f} |\n"
                        md += f"| Prediction Range | {target_stats['range']:.6f} |\n"
                        md += f"| Median | {target_stats['median']:.6f} |\n\n"
                return md
        else:
            # Classification statistics
            md = "\n#### Classification Prediction Statistics\n\n"
            md += "| Statistic | Value |\n"
            md += "|-----------|-------|\n"
            md += f"| Total Predictions | {stats['total_predictions']:,} |\n"
            md += f"| Unique Classes | {stats['unique_classes']} |\n"
            
            if stats['most_common_class']:
                most_common = stats['most_common_class']
                md += f"| Most Common Class | {most_common[0]} ({most_common[1]} predictions) |\n"
            
            md += "\n**Class Distribution:**\n\n"
            md += "| Class | Count | Percentage |\n"
            md += "|-------|-------|------------|\n"
            
            total = stats['total_predictions']
            for class_label, count in sorted(stats['class_distribution'].items()):
                percentage = (count / total) * 100 if total > 0 else 0
                md += f"| {class_label} | {count} | {percentage:.1f}% |\n"
            
            return md

    def _analyze_confidence_scores(self, confidence_scores: List) -> Dict[str, Any]:
        """Analyze confidence scores and return statistics."""
        if not confidence_scores:
            return {}
        
        # Handle both single values and arrays
        if isinstance(confidence_scores[0], (list, np.ndarray)):
            # Multi-target confidence scores - take mean across targets
            flat_scores = [np.mean(score) for score in confidence_scores]
        else:
            flat_scores = confidence_scores
        
        scores_array = np.array(flat_scores)
        total_count = len(scores_array)
        
        high_count = np.sum(scores_array >= 0.8)
        medium_count = np.sum((scores_array >= 0.5) & (scores_array < 0.8))
        low_count = np.sum(scores_array < 0.5)
        
        return {
            'mean': float(np.mean(scores_array)),
            'std': float(np.std(scores_array)),
            'min': float(np.min(scores_array)),
            'max': float(np.max(scores_array)),
            'high_count': int(high_count),
            'medium_count': int(medium_count),
            'low_count': int(low_count),
            'high_pct': (high_count / total_count) * 100 if total_count > 0 else 0,
            'medium_pct': (medium_count / total_count) * 100 if total_count > 0 else 0,
            'low_pct': (low_count / total_count) * 100 if total_count > 0 else 0
        }