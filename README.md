# XGBOOST MCP Tool

A comprehensive XGBOOST machine learning tool designed for MCP protocol integration, providing high-performance gradient boosting capabilities for AI applications and automated workflows.

## ğŸ“‹ Overview

This project adapts existing Random Forest MCP tool architecture to implement XGBOOST functionality while maintaining complete feature compatibility and user experience consistency.

## ğŸš€ Features

- **High Performance**: XGBOOST gradient boosting algorithm with excellent ML competition performance
- **Flexibility**: Support for regression, classification, and multi-target tasks
- **Interpretability**: Rich feature importance analysis capabilities
- **User-Friendly**: Consistent interface design with existing tools

### Core MCP Tool Functions

- `train_xgboost_regressor` - Train XGBOOST regression models with multi-target support
- `train_xgboost_classifier` - Train XGBOOST classification models
- `predict_from_file` - Batch prediction capabilities
- `predict_from_values` - Real-time predictions
- `analyze_global_feature_importance` - Global feature importance analysis
- `analyze_local_feature_importance` - Local feature importance analysis
- Model management functions (list, info, delete)

## ğŸ› ï¸ Installation

### Prerequisites

- Python >= 3.8
- UV package manager

### Setup

1. Clone the repository:
```bash
git clone <repository-url>
cd MCP-XGBOOST-Tool
```

2. Create virtual environment:
```bash
uv venv
```

3. Activate virtual environment:
```bash
# Windows
.venv\Scripts\activate
# Linux/Mac
source .venv/bin/activate
```

4. Install dependencies:
```bash
uv pip install -r requirements.txt
```

## ğŸ“¦ Dependencies

- xgboost >= 2.0.0
- scikit-learn >= 1.3.0
- pandas >= 2.0.0
- numpy >= 1.24.0
- optuna >= 3.4.0
- shap >= 0.42.0
- matplotlib >= 3.7.0
- seaborn >= 0.12.0
- fastapi >= 0.104.0
- mcp >= 1.0.0
- joblib >= 1.3.0
- jinja2 >= 3.1.0

## ğŸ—ï¸ Project Structure

```
src/mcp_xgboost_tool/
â”œâ”€â”€ mcp_server.py                   # MCPæœåŠ¡å™¨ (æ ¸å¿ƒæ”¹é€ )
â”œâ”€â”€ xgboost_wrapper.py              # XGBOOSTåŒ…è£…ç±» (æ–°å»º)
â”œâ”€â”€ training.py                     # è®­ç»ƒå¼•æ“ (æ”¹é€ )
â”œâ”€â”€ prediction.py                   # é¢„æµ‹å¼•æ“ (è½»å¾®æ”¹é€ )
â”œâ”€â”€ feature_importance.py           # ç‰¹å¾é‡è¦æ€§ (æ”¹é€ )
â”œâ”€â”€ local_feature_importance.py     # å±€éƒ¨ç‰¹å¾é‡è¦æ€§ (æ”¹é€ )
â”œâ”€â”€ hyperparameter_optimizer.py     # è¶…å‚æ•°ä¼˜åŒ– (æ”¹é€ )
â””â”€â”€ model_manager.py               # æ¨¡å‹ç®¡ç† (è½»å¾®æ”¹é€ )
```

## ğŸ¯ Development Roadmap

### Phase 1: Core Algorithm Replacement (1-2 weeks)
- [x] Project setup
- [ ] Create XGBoostWrapper class
- [ ] Update MCP server functions
- [ ] Refactor TrainingEngine

### Phase 2: Hyperparameter Optimization & Feature Analysis (1-2 weeks)
- [ ] Update HyperparameterOptimizer for XGBOOST parameters
- [ ] Enhance FeatureImportanceAnalyzer
- [ ] Update cross-validation strategies
- [ ] Enhance evaluation metrics

### Phase 3: Reports & Visualization (1 week)
- [ ] Update HTML report generator
- [ ] Adjust visualization components
- [ ] Update academic report format

### Phase 4: Testing & Optimization (1 week)
- [ ] Complete functional testing
- [ ] Performance optimization
- [ ] Edge case handling
- [ ] Documentation completion

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Run tests
5. Submit a pull request

## ğŸ“Š Performance Requirements

- Training speed: No slower than Random Forest on most datasets
- Prediction accuracy: Superior to Random Forest on benchmark datasets
- Memory usage: Reasonable memory footprint
- API response time: <500ms for small-scale predictions

## ğŸ”§ Development Status

This project is currently in development. Check the roadmap above for current progress.
