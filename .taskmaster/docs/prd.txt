# XGBOOST MCP Tool - 产品需求文档 (PRD)

## 1. 项目概述

### 1.1 项目背景
基于现有的随机森林MCP工具，开发一个功能完整的XGBOOST机器学习工具，支持MCP协议，为AI应用和自动化工作流程提供高性能的梯度提升算法能力。

### 1.2 项目目标
- 将现有随机森林MCP工具架构改造为XGBOOST实现
- 保持现有工具的完整功能特性和API接口
- 利用XGBOOST的高性能和先进特性
- 提供与随机森林工具相同的用户体验

### 1.3 核心价值
- **高性能**: XGBOOST的梯度提升算法在许多ML竞赛中表现优异
- **灵活性**: 支持回归、分类、多目标等多种任务类型
- **可解释性**: 提供丰富的特征重要性分析功能
- **易用性**: 保持与现有工具相同的接口设计

## 2. 功能需求

### 2.1 核心MCP工具函数

#### 2.1.1 train_xgboost_regressor
训练XGBOOST回归模型，支持多目标回归。
包含参数：data_source, target_dimension, optimize_hyperparameters, n_trials, cv_folds, scoring_metric, validate_data, save_model, apply_preprocessing, scaling_method, early_stopping_rounds, eval_metric

XGBOOST特有参数:
- early_stopping_rounds: 早停轮次
- eval_metric: 评估指标
- learning_rate: 学习率
- max_depth: 树的最大深度
- n_estimators: 提升轮数
- subsample: 样本采样比例
- colsample_bytree: 特征采样比例

#### 2.1.2 train_xgboost_classifier
训练XGBOOST分类模型，支持二分类、多分类任务。
包含参数：data_source, target_dimension, optimize_hyperparameters, n_trials, cv_folds, scoring_metric, validate_data, save_model, early_stopping_rounds, eval_metric

#### 2.1.3 predict_from_file
批量预测功能，包含XGBOOST增强特性：
- prediction_intervals: 预测区间计算
- 更精确的置信度计算

#### 2.1.4 predict_from_values
实时预测功能，保持与现有接口相同，增加XGBOOST特有的预测特性。

#### 2.1.5 analyze_global_feature_importance
全局特征重要性分析，支持XGBOOST特有分析类型：
- gain: 基于增益的重要性
- cover: 基于覆盖度的重要性  
- weight: 基于权重的重要性
- permutation: 排列重要性
- shap: SHAP值分析

#### 2.1.6 analyze_local_feature_importance
局部特征重要性分析，保持现有接口，增强SHAP分析能力。

### 2.2 模型管理功能
- list_models: 列出所有XGBOOST模型
- get_model_info: 获取模型详细信息
- delete_model: 删除模型

## 3. 技术架构改造

### 3.1 核心模块改造

#### 3.1.1 XGBoostWrapper (替代RandomForestWrapper)
XGBOOST算法的包装类，主要功能：
- 自动任务类型检测
- 支持回归和分类
- 内置特征重要性计算
- 交叉验证支持
- 早停机制

#### 3.1.2 TrainingEngine改造
- 集成XGBOOST超参数优化
- 支持XGBOOST特有的评估指标
- 增加早停和学习曲线监控
- 支持GPU训练(可选)

#### 3.1.3 HyperparameterOptimizer改造
XGBOOST参数空间优化，支持参数：
- n_estimators: 提升轮数 (50-1000)
- max_depth: 最大深度 (3-10)
- learning_rate: 学习率 (0.01-0.3)
- subsample: 样本采样比例 (0.6-1.0)
- colsample_bytree: 特征采样比例 (0.6-1.0)
- reg_alpha: L1正则化参数
- reg_lambda: L2正则化参数
- min_child_weight: 最小子节点权重 (1-10)

### 3.2 依赖更新
项目依赖包括：
- xgboost>=2.0.0
- scikit-learn>=1.3.0
- pandas>=2.0.0
- numpy>=1.24.0
- optuna>=3.4.0
- shap>=0.42.0
- matplotlib>=3.7.0
- seaborn>=0.12.0
- fastapi>=0.104.0
- mcp>=1.0.0
- joblib>=1.3.0
- jinja2>=3.1.0

### 3.3 文件结构改造
核心文件包括：
- mcp_server.py: MCP服务器 (核心改造)
- xgboost_wrapper.py: XGBOOST包装类 (新建)
- training.py: 训练引擎 (改造)
- prediction.py: 预测引擎 (轻微改造)
- feature_importance.py: 特征重要性 (改造)
- local_feature_importance.py: 局部特征重要性 (改造)
- hyperparameter_optimizer.py: 超参数优化 (改造)
- model_manager.py: 模型管理 (轻微改造)

## 4. 实现计划

### 4.1 阶段一: 核心算法替换 (1-2周)
1. 创建 XGBoostWrapper 类
2. 更新 mcp_server.py 中的工具函数
3. 改造 TrainingEngine 支持XGBOOST
4. 基础功能测试

### 4.2 阶段二: 超参数优化和特征分析 (1-2周)
1. 更新 HyperparameterOptimizer 支持XGBOOST参数
2. 改造 FeatureImportanceAnalyzer 支持XGBOOST特有分析
3. 更新交叉验证策略
4. 增强评估指标

### 4.3 阶段三: 报告和可视化 (1周)
1. 更新HTML报告生成器
2. 调整可视化组件
3. 更新学术报告格式
4. 完善文档

### 4.4 阶段四: 测试和优化 (1周)
1. 完整功能测试
2. 性能优化
3. 边界情况处理
4. 文档完善

## 5. 关键差异点

### 5.1 算法差异
随机森林 vs XGBOOST:
- 算法类型: Bagging vs Boosting
- 训练方式: 并行 vs 串行
- 过拟合控制: 随机采样 vs 正则化+早停
- 特征重要性: MDI, 排列 vs Gain, Cover, Weight
- 超参数复杂度: 较简单 vs 较复杂

### 5.2 实现差异
- 模型保存: XGBOOST使用.json或.ubj格式
- 评估指标: 增加XGBOOST特有指标
- 早停机制: 需要验证集和早停轮次
- GPU支持: 可选的GPU加速训练

## 6. 质量保证

### 6.1 测试策略
- 单元测试覆盖所有核心模块
- 集成测试验证端到端流程
- 性能基准测试对比随机森林
- 边界条件和异常处理测试

### 6.2 性能要求
- 训练速度: 在大多数数据集上不低于随机森林
- 预测精度: 在基准数据集上优于随机森林
- 内存使用: 合理的内存占用
- API响应时间: <500ms for小规模预测

## 7. 风险和缓解

### 7.1 技术风险
- XGBOOST版本兼容性: 固定版本依赖
- 超参数调优复杂: 提供合理默认值
- 内存占用: 实现内存监控和优化

### 7.2 迁移风险
- API兼容性: 保持向后兼容
- 数据格式: 确保数据处理一致性
- 模型格式: 提供迁移工具

## 8. 成功指标
- 所有MCP工具函数正常工作
- 超参数优化收敛稳定
- 特征重要性分析准确
- 报告和可视化完整
- 性能达到预期
- 测试覆盖率 >90%

这个PRD为XGBOOST MCP工具的开发提供了完整的指导框架，确保在保持现有架构优势的同时，充分利用XGBOOST的强大功能。 